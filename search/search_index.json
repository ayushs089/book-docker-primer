{
    "docs": [
        {
            "location": "/", 
            "text": "Zero to Docker\n\n\nWelcome to Zero to Docker program by School of Devops\n\n\nThis is a Lab Guide which goes along with this Docker course by School of Devops.\n\n\nFor information about the devops trainign courses visit \nschoolofdevops.com\n.\n\n\nTeam\n\n\n\n\nGourav Shah\n\n\nVijayboopathy", 
            "title": "Home"
        }, 
        {
            "location": "/#zero-to-docker", 
            "text": "Welcome to Zero to Docker program by School of Devops  This is a Lab Guide which goes along with this Docker course by School of Devops.  For information about the devops trainign courses visit  schoolofdevops.com .", 
            "title": "Zero to Docker"
        }, 
        {
            "location": "/#team", 
            "text": "Gourav Shah  Vijayboopathy", 
            "title": "Team"
        }, 
        {
            "location": "/chapter4-getting_started/", 
            "text": "Getting Started with Docker\n\n\nIn this chapter, we are going to learn about docker shell, the command line utility and how to use it to\nlaunch containers. We will also learn what it means to run a container, its lifecycle and perform basic\noperations such as creating, starting, stopping, removing, pausing containers and checking the status etc.\n\n\nUsing docker cli\n\n\nWe can use docker cli to interact with docker daemon. Various functions of docker command is given below. Try this yourself by runnig \n$sudo docker\n command  \n\n\ndocker\n\n\n\n\n[Output]  \n\n\nUsage: docker [OPTIONS] COMMAND [arg...]\n       docker [ --help | -v | --version ]\n\nA self-sufficient runtime for containers.\n\nOptions:\n\n  --config=~/.docker              Location of client config files\n  -D, --debug                     Enable debug mode\n  -H, --host=[]                   Daemon socket(s) to connect to\n  -h, --help                      Print usage\n  -l, --log-level=info            Set the logging level\n  --tls                           Use TLS; implied by --tlsverify\n  --tlscacert=~/.docker/ca.pem    Trust certs signed only by this CA\n  --tlscert=~/.docker/cert.pem    Path to TLS certificate file\n  --tlskey=~/.docker/key.pem      Path to TLS key file\n  --tlsverify                     Use TLS and verify the remote\n  -v, --version                   Print version information and quit\n\nCommands:\n    attach    Attach to a running container\n    build     Build an image from a Dockerfile\n    commit    Create a new image from a container's changes\n    cp        Copy files/folders between a container and the local filesystem\n    create    Create a new container\n    diff      Inspect changes on a container's filesystem\n    events    Get real time events from the server\n    exec      Run a command in a running container\n    export    Export a container's filesystem as a tar archive\n    history   Show the history of an image\n    images    List images\n    import    Import the contents from a tarball to create a filesystem image\n    info      Display system-wide information\n    inspect   Return low-level information on a container, image or task\n    kill      Kill one or more running containers\n    load      Load an image from a tar archive or STDIN\n    login     Log in to a Docker registry.\n    logout    Log out from a Docker registry.\n    logs      Fetch the logs of a container\n    network   Manage Docker networks\n    node      Manage Docker Swarm nodes\n    pause     Pause all processes within one or more containers\n    port      List port mappings or a specific mapping for the container\n    ps        List containers\n    pull      Pull an image or a repository from a registry\n    push      Push an image or a repository to a registry\n    rename    Rename a container\n    restart   Restart a container\n    rm        Remove one or more containers\n    rmi       Remove one or more images\n    run       Run a command in a new container\n    save      Save one or more images to a tar archive (streamed to STDOUT by default)\n    search    Search the Docker Hub for images\n    service   Manage Docker services\n    start     Start one or more stopped containers\n    stats     Display a live stream of container(s) resource usage statistics\n    stop      Stop one or more running containers\n    swarm     Manage Docker Swarm\n    tag       Tag an image into a repository\n    top       Display the running processes of a container\n    unpause   Unpause all processes within one or more containers\n    update    Update configuration of one or more containers\n    version   Show the Docker version information\n    volume    Manage Docker volumes\n    wait      Block until a container stops, then print its exit code\n\n\n\n\n\nGetting Information about Docker Setup\n\n\nWe can get the information about our Docker setup in several ways. Namely,  \n\n\ndocker -v  \n\ndocker version  \n\ndocker system info\n\n\n\n\n[Output of \ndocker -v\n]  \n\n\nDocker version 18.03.1-ce, build 9ee9f40\n\n\n\n\n[Output of \ndocker version\n]  \n\n\nClient:\n Version:      18.03.1-ce\n API version:  1.37\n Go version:   go1.9.5\n Git commit:   9ee9f40\n Built:        Thu Apr 26 07:18:46 2018\n OS/Arch:      linux/amd64\n Experimental: false\n Orchestrator: swarm\n\nServer:\n Engine:\n  Version:      18.03.1-ce\n  API version:  1.37 (minimum version 1.12)\n  Go version:   go1.9.5\n  Git commit:   9ee9f40\n  Built:        Thu Apr 26 07:16:59 2018\n  OS/Arch:      linux/amd64\n  Experimental: false\n\n\n\n\nThe \ndocker system info\n command gives a lot of useful information like total number of containers and images along with information about host resource utilization  etc.\n\n\nLaunching our first container\n\n\nNow we have a basic understanding of docker command and sub commands, let us dive straight into launching our very first \ncontainer\n  \n\n\ndocker run alpine:3.4 uptime\n\n\n\n\nWhere,\n  * we are using docker \nclient\n to\n  * run a application/command \nuptime\n using\n\n  * an image by name \nalpine:3.4\n\n\n[Output]  \n\n\nUnable to find image 'alpine:3.4' locally\n3.4: Pulling from library/alpine\n81033e7c1d6a: Pull complete\nDigest: sha256:2532609239f3a96fbc30670716d87c8861b8a1974564211325633ca093b11c0b\nStatus: Downloaded newer image for alpine:3.4\n\n 15:24:34 up  7:36,  load average: 0.00, 0.03, 0.04\n\n\n\n\nWhat happened?\n\nThis command will\n\n  * Pull the \nalpine\n image file from \ndocker hub\n, a cloud registry\n  * Create a runtime environment/ container with the above image \n\n  * Launch a program (called \nuptime\n) inside that container\n\n  * Stream that output to the terminal\n\n  * Stop the container once the program is exited\n\n\nWhere did my container go?\n  \n\n\ndocker container  ps\n\ndocker container  ps -l\n\n\n\n\nThe point here to remember is that, when that executable stops running inside the container, the container itself will stop\n\nThis process will further be explained under the \nlifecycle of a container\n topic.\n\n\nLet's see what happens when we run that command again,  \n\n\n[Output]  \n\n\ndocker run alpine uptime\n 07:48:06 up  3:15,  load average: 0.00, 0.00, 0.00\n\n\n\n\nNow docker no longer pulls the image again from registry, because \nit has stored the image locally\n from the previous run\n\nSo once an image is pulled, we can make use of that image to create and run as many container as we want without the need of downloading the image again. However it has created a new instance of the iamge/container.\n\n\nChecking Status of the containers\n\n\nWe have understood how docker run commands works. But what if you want to see list of running containers and history of containers that had run and exited? This can be done by executing the following commands  \n\n\ndocker ps\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\n\n\n\nThis command doesn't give us any information. Because, \ndocker ps\n command will only show list of container(s) which are \nrunning\n  \n\n\ndocker ps -l\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS                          PORTS               NAMES\n988f4d90d604        alpine              \nuptime\n            About a minute ago   Exited (0) About a minute ago                       fervent_hypatia\n\n\n\n\nthe \n-l\n flag shows the last run container along with other details like image it used, command it executed, return code of that command, etc.,  \n\n\ndocker ps -n 2\n\n\n\n\n[Output]  \n\n\nNAMES\n988f4d90d604        alpine              \nuptime\n            About a minute ago   Exited (0) About a minute ago                       fervent_hypatia\nacea3023dca4        alpine              \nuptime\n            3 minutes ago        Exited (0) 3 minutes ago                            mad_darwin\n\n\n\n\nDocker gives us the flexibility to show the desirable number of last run containers. This can be achieved by using \n-n #no_of_results\n flag  \n\n\ndocker ps -a\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE                        COMMAND                  CREATED              STATUS                          PORTS                  NAMES\n988f4d90d604        alpine                       \nuptime\n                 About a minute ago   Exited (0) About a minute ago                          fervent_hypatia\nacea3023dca4        alpine                       \nuptime\n                 4 minutes ago        Exited (0) 4 minutes ago                               mad_darwin\n60ffa94e69ec        ubuntu:14.04.3               \nbash\n                   27 hours ago         Exited (0) 26 hours ago                                infallible_meninsky\ndd75c04e7d2b        schoolofdevops/ghost:0.3.1   \n/entrypoint.sh npm s\n   4 days ago           Exited (0) 3 days ago                                  kickass_bardeen\nc082972f66d6        schoolofdevops/ghost:0.3.1   \n/entrypoint.sh npm s\n   4 days ago           Exited (0) 3 days ago           0.0.0.0:80-\n2368/tcp   sodcblog\n\n\n\n\n\nThis command will show all the container we have run so far.  \n\n\nRunning Containers in Interactive Mode\n\n\nWe can interact with docker containers by giving -it flags at the run time. These flags stand for\n\n  * i - Interactive\n\n  * t - tty\n\n\ndocker run -it alpine sh\n\n\n\n\n[Output]  \n\n\nUnable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\nff3a5c916c92: Already exists\nDigest: sha256:7df6db5aa61ae9480f52f0b3a06a140ab98d427f86d8d5de0bedab9b8df6b1c0\nStatus: Downloaded newer image for alpine:latest\n/ #\n\n\n\n\nAs you see, we have landed straight into \nsh\n shell of that container. This is the result of using \n-it\n flags and mentioning that container to run the \nsh\n shell. Don't try to exit that container yet. We have to execute some other commands in it to understand the next topic  \n\n\nNamespaced:\n\n\nLike a full fledged OS, Docker container has its own namespaces\n\nThis enables Docker container to isolate itself from the host as well as other containers  \n\n\nRun the following commands and see that alpine container has its own namespaces and not inheriting much from \nhost OS\n  \n\n\n[Command]  \n\n\ncat /etc/issue\n\n\n\n\n[Output]  \n\n\nWelcome to Alpine Linux 3.4\nKernel \\r on an \\m (\\l)\n\n\n\n\n[Command]  \n\n\nps aux\n\n\n\n\n[Output]  \n\n\nPID   USER     TIME   COMMAND\n    1 root       0:00 sh\n    6 root       0:00 ps aux\n\n\n\n\n[Command]  \n\n\nifconfig\n\n\n\n\n[Output]  \n\n\neth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:02\n          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::42:acff:fe11:2%32640/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:8 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)\n\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1%32640/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n\n\n\n\n[Command]  \n\n\nhostname\n\n\n\n\n[Output]   \n\n\nae84d253ecb5\n\n\n\n\nShared:\n\nWe have understood that containers have their own namespaces. But will they share something to some extent? the answer is \nYES\n. Let's run the following commands on both the container and the host machine  \n\n\n[Command]  \n\n\nuname -a\n\n\n\n\n[Output - \ncontainer\n]  \n\n\nLinux ae84d253ecb5 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 Linux\n\n\n\n\n\n[Output - \nhostmachine\n]  \n\n\nLinux dockerserver 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\n\n\n\nAs you can see, the container uses the same Linux Kernel from the host machine. Just like \nuname\n command, the following commands share the same information as well. In order to avoid repetition, we will see the output of container alone.\n\n\n[Command]  \n\n\ndate  \n\n\n\n\n[Output]    \n\n\nWed Sep 14 18:21:25 UTC 2016\n\n\n\n\n[Command]  \n\n\ncat /proc/cpuinfo\n\n\n\n\n[Output]    \n\n\nprocessor       : 0\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 94\nmodel name      : Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz\nstepping        : 3\ncpu MHz         : 2592.002\ncache size      : 6144 KB\nphysical id     : 0\nsiblings        : 1\ncore id         : 0\ncpu cores       : 1\napicid          : 0\ninitial apicid  : 0\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 22\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx rdrand hypervisor lahf_lm abm 3dnowprefetch rdseed clflushopt\nbogomips        : 5184.00\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 39 bits physical, 48 bits virtual\npower management:\n\n\n\n\n[Command]  \n\n\nfree\n\n\n\n\n[Output]  \n\n\ntotal       used       free     shared    buffers     cached\nMem:       1884176     650660    1233516          0       1860     473248\n-/+ buffers/cache:     175552    1708624\nSwap:      1048572          0    1048572\n\n\n\n\n\nNow exit out of that container by running \nexit\n or by pressing \nctrl+d\n  \n\n\nMaking Containers Persist\n\n\nRunning Containers in Detached Mode\n\n\nSo far, we have run the containers interactively. But this is not always the case. Sometimes you may want to start a container  without interacting with it. This can be achieved by using \n\"detached mode\"\n (\n-d\n) flag. Hence the container will launch the deafault application inside and run in the background. This saves a lot of time, we don't have to wait till the applications launches successfully. It will happen behind the screen. Let us run the following command to see this in action  \n\n\n[Command]  \n\n\ndocker run -idt schoolofdevops/loop program\n\n\n\n\n-d , --detach : detached mode  \n\n\n[Output]  \n\n\n2533adf280ac4838b446e4ee1151f52793e6ac499d2e631b2c752459bb18ad5f\n\n\n\n\nThis will run the container in detached mode. We are only given with full container id as the output  \n\n\nLet us check whether this container is running or not\n\n[Command]  \n\n\ndocker ps\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS               NAMES\n2533adf280ac        schoolofdevops/loop   \nprogram\n           37 seconds ago      Up 36 seconds                           prickly_bose\n\n\n\n\nAs we can see in the output, the container is running in the background  \n\n\nChecking Logs\n\n\nTo check the logs, find out the container id/name and run the following commands, replacing 08f0242aa61c with your container id\n\n\n[Commands]  \n\n\ndocker container ps\n\ndocker container logs 08f0242aa61c\n\ndocker container logs -f  08f0242aa61c\n\n\n\n\nConnecting to running container to execute commands\n\n\nWe can connect to the containers which are running in detached mode by using these following commands\n\n[Command]  \n\n\ndocker exec -it 2533adf280ac sh\n\n\n\n\n[Output]  \n\n\n/ #\n\n\n\n\nYou could try running any commands on the shell\ne.g.\n\n\napk update\napk add vim\nps aux\n\n\n\n\nNow exit the container.  \n\n\nPausing Running Container\n\n\nJust like in a video, it is easy to pause and unpause the running container\n\n[Command]  \n\n\ndocker pause 2533adf280ac\n\n\n\n\nAfter running pause command, run docker ps again to check the container status  \n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS                  PORTS               NAMES\n2533adf280ac        schoolofdevops/loop   \nprogram\n           2 minutes ago       Up 2 minutes (Paused)                       prickly_bose\n\n\n\n\nUnpausing the paused container\n\n\nThis can be achieved by executing following command  \n\n\n[Command]  \n\n\ndocker unpause\n\n\n\n\nRun docker ps to verify the changes  \n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS               NAMES\n2533adf280ac        schoolofdevops/loop   \nprogram\n           6 minutes ago       Up 6 minutes                            prickly_bose\n\n\n\n\nCreating and Starting a Container instead of Running\n\n\ndocker \nrun\n command will create a container and start that container simultaneously. However docker gives you the granularity to create a container and not to run it at the time of creation. However, This container can be started by using \nstart\n command  \n\n\n[Command]  \n\n\ndocker create alpine:3.4 sh\n\n\n\n\nRun \ndocker ps -l\n to see the status of the container  \n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n22146d15eb71        alpine:3.4          \nsh\n                31 seconds ago      Created                                 grave_leavitt\n\n\n\n\nIf you do \ndocker ps -l\n, you will find that container status to be \nCreated\n. Now lets start this container by executing,   \n\n\n[Command]  \n\n\ndocker start 22146d15eb71\n\n\n\n\nRun docker ps -l again to see the status change  \n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES\n22146d15eb71        alpine:3.4          \nsh\n                3 minutes ago       Exited (0) 2 minutes ago                      grave_leavitt\n\n\n\n\nThis command will start the container and exit right away we have not specified interactive mode in the command  \n\n\nCreating Pretty Reports with Formatters\n\n\ndocker ps --format \n{{.ID}}: {{.Status}}\n\n\n\n\n\n[Output]  \n\n\n2533adf280ac: Up 12 minutes\n\n\n\n\nChecking disk utilisation by\n\n\n\ndocker system df\n\n\n\n\n\n[output]\n\n\ndocker system df\nTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE\nImages              7                   5                   1.031GB             914.5MB (88%)\nContainers          8                   4                   27.97MB             27.73MB (99%)\nLocal Volumes       3                   2                   0B                  0B\nBuild Cache                                                 0B                  0B\n\n\n\n\nTo prune, you could possibly use\n\n\ndocker container prune\n\ndocker system prune\n\n\n\n\ne.g.\n\n\ndocker system prune\nWARNING! This will remove:\n        - all stopped containers\n        - all networks not used by at least one container\n        - all dangling images\n        - all build cache\nAre you sure you want to continue? [y/N]\n\n\n\n\nMake sure you understand what all will be removed before using this command.\n\n\nStopping and Removing Containers\n\n\nWe have learnt about interacting with a container, running a container, pausing and unpausing a container, creating and starting a container. But what if you want to stop the container or remove the container itself  \n\n\nStop a container\n\n\nA container can be stopped using \nstop\n command. This command will stop the application inside that container hence the container itself will be stopped. This command basically sends a \nSIGTERM\n signal to the container (graceful shutdown)  \n\n\n[Command]  \n\n\ndocker stop 2533adf280ac\n\n\n\n\n[Output]  \n\n\n2533adf280ac\n\n\n\n\nKill a container\n\n\nThis command will send \nSIGKILL\n signal and kills the container ungracefully  \n\n\n[Command]  \n\n\ndocker kill 590e7060743a\n\n\n\n\n[Output]  \n\n\n590e7060743a\n\n\n\n\nIf you want to remove a container, then execute the following command. Before running this command, run docker ps -a to see the list of pre run containers. Choose a container of your wish and then execute docker rm command. Then run docker ps -a again to check the removed container list or not  \n\n\n[Command]  \n\n\ndocker rm 590e7060743a\n\n\n\n\n[Output]  \n\n\n590e7060743a", 
            "title": "Getting Started with Docker"
        }, 
        {
            "location": "/chapter4-getting_started/#getting-started-with-docker", 
            "text": "In this chapter, we are going to learn about docker shell, the command line utility and how to use it to\nlaunch containers. We will also learn what it means to run a container, its lifecycle and perform basic\noperations such as creating, starting, stopping, removing, pausing containers and checking the status etc.", 
            "title": "Getting Started with Docker"
        }, 
        {
            "location": "/chapter4-getting_started/#using-docker-cli", 
            "text": "We can use docker cli to interact with docker daemon. Various functions of docker command is given below. Try this yourself by runnig  $sudo docker  command    docker  [Output]    Usage: docker [OPTIONS] COMMAND [arg...]\n       docker [ --help | -v | --version ]\n\nA self-sufficient runtime for containers.\n\nOptions:\n\n  --config=~/.docker              Location of client config files\n  -D, --debug                     Enable debug mode\n  -H, --host=[]                   Daemon socket(s) to connect to\n  -h, --help                      Print usage\n  -l, --log-level=info            Set the logging level\n  --tls                           Use TLS; implied by --tlsverify\n  --tlscacert=~/.docker/ca.pem    Trust certs signed only by this CA\n  --tlscert=~/.docker/cert.pem    Path to TLS certificate file\n  --tlskey=~/.docker/key.pem      Path to TLS key file\n  --tlsverify                     Use TLS and verify the remote\n  -v, --version                   Print version information and quit\n\nCommands:\n    attach    Attach to a running container\n    build     Build an image from a Dockerfile\n    commit    Create a new image from a container's changes\n    cp        Copy files/folders between a container and the local filesystem\n    create    Create a new container\n    diff      Inspect changes on a container's filesystem\n    events    Get real time events from the server\n    exec      Run a command in a running container\n    export    Export a container's filesystem as a tar archive\n    history   Show the history of an image\n    images    List images\n    import    Import the contents from a tarball to create a filesystem image\n    info      Display system-wide information\n    inspect   Return low-level information on a container, image or task\n    kill      Kill one or more running containers\n    load      Load an image from a tar archive or STDIN\n    login     Log in to a Docker registry.\n    logout    Log out from a Docker registry.\n    logs      Fetch the logs of a container\n    network   Manage Docker networks\n    node      Manage Docker Swarm nodes\n    pause     Pause all processes within one or more containers\n    port      List port mappings or a specific mapping for the container\n    ps        List containers\n    pull      Pull an image or a repository from a registry\n    push      Push an image or a repository to a registry\n    rename    Rename a container\n    restart   Restart a container\n    rm        Remove one or more containers\n    rmi       Remove one or more images\n    run       Run a command in a new container\n    save      Save one or more images to a tar archive (streamed to STDOUT by default)\n    search    Search the Docker Hub for images\n    service   Manage Docker services\n    start     Start one or more stopped containers\n    stats     Display a live stream of container(s) resource usage statistics\n    stop      Stop one or more running containers\n    swarm     Manage Docker Swarm\n    tag       Tag an image into a repository\n    top       Display the running processes of a container\n    unpause   Unpause all processes within one or more containers\n    update    Update configuration of one or more containers\n    version   Show the Docker version information\n    volume    Manage Docker volumes\n    wait      Block until a container stops, then print its exit code", 
            "title": "Using docker cli"
        }, 
        {
            "location": "/chapter4-getting_started/#getting-information-about-docker-setup", 
            "text": "We can get the information about our Docker setup in several ways. Namely,    docker -v  \n\ndocker version  \n\ndocker system info  [Output of  docker -v ]    Docker version 18.03.1-ce, build 9ee9f40  [Output of  docker version ]    Client:\n Version:      18.03.1-ce\n API version:  1.37\n Go version:   go1.9.5\n Git commit:   9ee9f40\n Built:        Thu Apr 26 07:18:46 2018\n OS/Arch:      linux/amd64\n Experimental: false\n Orchestrator: swarm\n\nServer:\n Engine:\n  Version:      18.03.1-ce\n  API version:  1.37 (minimum version 1.12)\n  Go version:   go1.9.5\n  Git commit:   9ee9f40\n  Built:        Thu Apr 26 07:16:59 2018\n  OS/Arch:      linux/amd64\n  Experimental: false  The  docker system info  command gives a lot of useful information like total number of containers and images along with information about host resource utilization  etc.", 
            "title": "Getting Information about Docker Setup"
        }, 
        {
            "location": "/chapter4-getting_started/#launching-our-first-container", 
            "text": "Now we have a basic understanding of docker command and sub commands, let us dive straight into launching our very first  container     docker run alpine:3.4 uptime  Where,\n  * we are using docker  client  to\n  * run a application/command  uptime  using \n  * an image by name  alpine:3.4  [Output]    Unable to find image 'alpine:3.4' locally\n3.4: Pulling from library/alpine\n81033e7c1d6a: Pull complete\nDigest: sha256:2532609239f3a96fbc30670716d87c8861b8a1974564211325633ca093b11c0b\nStatus: Downloaded newer image for alpine:3.4\n\n 15:24:34 up  7:36,  load average: 0.00, 0.03, 0.04  What happened? \nThis command will \n  * Pull the  alpine  image file from  docker hub , a cloud registry\n  * Create a runtime environment/ container with the above image  \n  * Launch a program (called  uptime ) inside that container \n  * Stream that output to the terminal \n  * Stop the container once the program is exited  Where did my container go?     docker container  ps\n\ndocker container  ps -l  The point here to remember is that, when that executable stops running inside the container, the container itself will stop \nThis process will further be explained under the  lifecycle of a container  topic.  Let's see what happens when we run that command again,    [Output]    docker run alpine uptime\n 07:48:06 up  3:15,  load average: 0.00, 0.00, 0.00  Now docker no longer pulls the image again from registry, because  it has stored the image locally  from the previous run \nSo once an image is pulled, we can make use of that image to create and run as many container as we want without the need of downloading the image again. However it has created a new instance of the iamge/container.", 
            "title": "Launching our first container"
        }, 
        {
            "location": "/chapter4-getting_started/#checking-status-of-the-containers", 
            "text": "We have understood how docker run commands works. But what if you want to see list of running containers and history of containers that had run and exited? This can be done by executing the following commands    docker ps  [Output]    CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES  This command doesn't give us any information. Because,  docker ps  command will only show list of container(s) which are  running     docker ps -l  [Output]    CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS                          PORTS               NAMES\n988f4d90d604        alpine               uptime             About a minute ago   Exited (0) About a minute ago                       fervent_hypatia  the  -l  flag shows the last run container along with other details like image it used, command it executed, return code of that command, etc.,    docker ps -n 2  [Output]    NAMES\n988f4d90d604        alpine               uptime             About a minute ago   Exited (0) About a minute ago                       fervent_hypatia\nacea3023dca4        alpine               uptime             3 minutes ago        Exited (0) 3 minutes ago                            mad_darwin  Docker gives us the flexibility to show the desirable number of last run containers. This can be achieved by using  -n #no_of_results  flag    docker ps -a  [Output]    CONTAINER ID        IMAGE                        COMMAND                  CREATED              STATUS                          PORTS                  NAMES\n988f4d90d604        alpine                        uptime                  About a minute ago   Exited (0) About a minute ago                          fervent_hypatia\nacea3023dca4        alpine                        uptime                  4 minutes ago        Exited (0) 4 minutes ago                               mad_darwin\n60ffa94e69ec        ubuntu:14.04.3                bash                    27 hours ago         Exited (0) 26 hours ago                                infallible_meninsky\ndd75c04e7d2b        schoolofdevops/ghost:0.3.1    /entrypoint.sh npm s    4 days ago           Exited (0) 3 days ago                                  kickass_bardeen\nc082972f66d6        schoolofdevops/ghost:0.3.1    /entrypoint.sh npm s    4 days ago           Exited (0) 3 days ago           0.0.0.0:80- 2368/tcp   sodcblog  This command will show all the container we have run so far.", 
            "title": "Checking Status of the containers"
        }, 
        {
            "location": "/chapter4-getting_started/#running-containers-in-interactive-mode", 
            "text": "We can interact with docker containers by giving -it flags at the run time. These flags stand for \n  * i - Interactive \n  * t - tty  docker run -it alpine sh  [Output]    Unable to find image 'alpine:latest' locally\nlatest: Pulling from library/alpine\nff3a5c916c92: Already exists\nDigest: sha256:7df6db5aa61ae9480f52f0b3a06a140ab98d427f86d8d5de0bedab9b8df6b1c0\nStatus: Downloaded newer image for alpine:latest\n/ #  As you see, we have landed straight into  sh  shell of that container. This is the result of using  -it  flags and mentioning that container to run the  sh  shell. Don't try to exit that container yet. We have to execute some other commands in it to understand the next topic    Namespaced:  Like a full fledged OS, Docker container has its own namespaces \nThis enables Docker container to isolate itself from the host as well as other containers    Run the following commands and see that alpine container has its own namespaces and not inheriting much from  host OS     [Command]    cat /etc/issue  [Output]    Welcome to Alpine Linux 3.4\nKernel \\r on an \\m (\\l)  [Command]    ps aux  [Output]    PID   USER     TIME   COMMAND\n    1 root       0:00 sh\n    6 root       0:00 ps aux  [Command]    ifconfig  [Output]    eth0      Link encap:Ethernet  HWaddr 02:42:AC:11:00:02\n          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::42:acff:fe11:2%32640/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:8 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:648 (648.0 B)  TX bytes:648 (648.0 B)\n\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1%32640/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)  [Command]    hostname  [Output]     ae84d253ecb5  Shared: \nWe have understood that containers have their own namespaces. But will they share something to some extent? the answer is  YES . Let's run the following commands on both the container and the host machine    [Command]    uname -a  [Output -  container ]    Linux ae84d253ecb5 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 Linux  [Output -  hostmachine ]    Linux dockerserver 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux  As you can see, the container uses the same Linux Kernel from the host machine. Just like  uname  command, the following commands share the same information as well. In order to avoid repetition, we will see the output of container alone.  [Command]    date    [Output]      Wed Sep 14 18:21:25 UTC 2016  [Command]    cat /proc/cpuinfo  [Output]      processor       : 0\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 94\nmodel name      : Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz\nstepping        : 3\ncpu MHz         : 2592.002\ncache size      : 6144 KB\nphysical id     : 0\nsiblings        : 1\ncore id         : 0\ncpu cores       : 1\napicid          : 0\ninitial apicid  : 0\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 22\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx rdrand hypervisor lahf_lm abm 3dnowprefetch rdseed clflushopt\nbogomips        : 5184.00\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 39 bits physical, 48 bits virtual\npower management:  [Command]    free  [Output]    total       used       free     shared    buffers     cached\nMem:       1884176     650660    1233516          0       1860     473248\n-/+ buffers/cache:     175552    1708624\nSwap:      1048572          0    1048572  Now exit out of that container by running  exit  or by pressing  ctrl+d", 
            "title": "Running Containers in Interactive Mode"
        }, 
        {
            "location": "/chapter4-getting_started/#making-containers-persist", 
            "text": "", 
            "title": "Making Containers Persist"
        }, 
        {
            "location": "/chapter4-getting_started/#running-containers-in-detached-mode", 
            "text": "So far, we have run the containers interactively. But this is not always the case. Sometimes you may want to start a container  without interacting with it. This can be achieved by using  \"detached mode\"  ( -d ) flag. Hence the container will launch the deafault application inside and run in the background. This saves a lot of time, we don't have to wait till the applications launches successfully. It will happen behind the screen. Let us run the following command to see this in action    [Command]    docker run -idt schoolofdevops/loop program  -d , --detach : detached mode    [Output]    2533adf280ac4838b446e4ee1151f52793e6ac499d2e631b2c752459bb18ad5f  This will run the container in detached mode. We are only given with full container id as the output    Let us check whether this container is running or not \n[Command]    docker ps  [Output]    CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS               NAMES\n2533adf280ac        schoolofdevops/loop    program            37 seconds ago      Up 36 seconds                           prickly_bose  As we can see in the output, the container is running in the background", 
            "title": "Running Containers in Detached Mode"
        }, 
        {
            "location": "/chapter4-getting_started/#checking-logs", 
            "text": "To check the logs, find out the container id/name and run the following commands, replacing 08f0242aa61c with your container id  [Commands]    docker container ps\n\ndocker container logs 08f0242aa61c\n\ndocker container logs -f  08f0242aa61c", 
            "title": "Checking Logs"
        }, 
        {
            "location": "/chapter4-getting_started/#connecting-to-running-container-to-execute-commands", 
            "text": "We can connect to the containers which are running in detached mode by using these following commands \n[Command]    docker exec -it 2533adf280ac sh  [Output]    / #  You could try running any commands on the shell\ne.g.  apk update\napk add vim\nps aux  Now exit the container.", 
            "title": "Connecting to running container to execute commands"
        }, 
        {
            "location": "/chapter4-getting_started/#pausing-running-container", 
            "text": "Just like in a video, it is easy to pause and unpause the running container \n[Command]    docker pause 2533adf280ac  After running pause command, run docker ps again to check the container status    [Output]    CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS                  PORTS               NAMES\n2533adf280ac        schoolofdevops/loop    program            2 minutes ago       Up 2 minutes (Paused)                       prickly_bose", 
            "title": "Pausing Running Container"
        }, 
        {
            "location": "/chapter4-getting_started/#unpausing-the-paused-container", 
            "text": "This can be achieved by executing following command    [Command]    docker unpause  Run docker ps to verify the changes    [Output]    CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS               NAMES\n2533adf280ac        schoolofdevops/loop    program            6 minutes ago       Up 6 minutes                            prickly_bose", 
            "title": "Unpausing the paused container"
        }, 
        {
            "location": "/chapter4-getting_started/#creating-and-starting-a-container-instead-of-running", 
            "text": "docker  run  command will create a container and start that container simultaneously. However docker gives you the granularity to create a container and not to run it at the time of creation. However, This container can be started by using  start  command    [Command]    docker create alpine:3.4 sh  Run  docker ps -l  to see the status of the container    [Output]    CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n22146d15eb71        alpine:3.4           sh                 31 seconds ago      Created                                 grave_leavitt  If you do  docker ps -l , you will find that container status to be  Created . Now lets start this container by executing,     [Command]    docker start 22146d15eb71  Run docker ps -l again to see the status change    [Output]    CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES\n22146d15eb71        alpine:3.4           sh                 3 minutes ago       Exited (0) 2 minutes ago                      grave_leavitt  This command will start the container and exit right away we have not specified interactive mode in the command", 
            "title": "Creating and Starting a Container instead of Running"
        }, 
        {
            "location": "/chapter4-getting_started/#creating-pretty-reports-with-formatters", 
            "text": "docker ps --format  {{.ID}}: {{.Status}}   [Output]    2533adf280ac: Up 12 minutes", 
            "title": "Creating Pretty Reports with Formatters"
        }, 
        {
            "location": "/chapter4-getting_started/#checking-disk-utilisation-by", 
            "text": "docker system df  [output]  docker system df\nTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE\nImages              7                   5                   1.031GB             914.5MB (88%)\nContainers          8                   4                   27.97MB             27.73MB (99%)\nLocal Volumes       3                   2                   0B                  0B\nBuild Cache                                                 0B                  0B  To prune, you could possibly use  docker container prune\n\ndocker system prune  e.g.  docker system prune\nWARNING! This will remove:\n        - all stopped containers\n        - all networks not used by at least one container\n        - all dangling images\n        - all build cache\nAre you sure you want to continue? [y/N]  Make sure you understand what all will be removed before using this command.", 
            "title": "Checking disk utilisation by"
        }, 
        {
            "location": "/chapter4-getting_started/#stopping-and-removing-containers", 
            "text": "We have learnt about interacting with a container, running a container, pausing and unpausing a container, creating and starting a container. But what if you want to stop the container or remove the container itself", 
            "title": "Stopping and Removing Containers"
        }, 
        {
            "location": "/chapter4-getting_started/#stop-a-container", 
            "text": "A container can be stopped using  stop  command. This command will stop the application inside that container hence the container itself will be stopped. This command basically sends a  SIGTERM  signal to the container (graceful shutdown)    [Command]    docker stop 2533adf280ac  [Output]    2533adf280ac", 
            "title": "Stop a container"
        }, 
        {
            "location": "/chapter4-getting_started/#kill-a-container", 
            "text": "This command will send  SIGKILL  signal and kills the container ungracefully    [Command]    docker kill 590e7060743a  [Output]    590e7060743a  If you want to remove a container, then execute the following command. Before running this command, run docker ps -a to see the list of pre run containers. Choose a container of your wish and then execute docker rm command. Then run docker ps -a again to check the removed container list or not    [Command]    docker rm 590e7060743a  [Output]    590e7060743a", 
            "title": "Kill a container"
        }, 
        {
            "location": "/chapter5-container_operations/", 
            "text": "Managing Containers - Learning about Common Container Operations\n\n\nIn the previous chapter, we have learnt about container lifecycle management including how to create, launch, connect to, stop and remove containers. In this chapter, we are going to learn how to launch a container with a pre built app image and how to access the app with published ports. We will also learn about common container operations such as inspecting container information, checking logs and performance stats, renaming and updating the properties of a container, limiting resources etc.  \n\n\nAs part of this lab, we are going to launch a python based webapp for a sample voting application.    \n\n\nLaunching a container with a pre built app image\n\n\nTo launch vote container run the following command. Don't bother about the new flag \n-P\n now. We will explain about that flag later in this chapter  \n\n\ndocker container run  -idt -P  schoolofdevops/vote\n\n\n\n\n[Output]  \n\n\nUnable to find image 'schoolofdevops/vote:latest' locally\nlatest: Pulling from schoolofdevops/vote\nDigest: sha256:9195942ea654fa8d8aeb37900be5619215c08e7e1bef0b7dfe4c04a9cc20a8c2\nStatus: Downloaded newer image for schoolofdevops/vote:latest\n7d58ecc05754b5fd192c4ecceae334ac22565684c6923ea332bff5c88e3fca2b\n\n\n\n\nLets check the status of the container  \n\n\ndocker ps -l\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                   NAMES\n7d58ecc05754        schoolofdevops/vote   \ngunicorn app:app -b\u2026\n   27 seconds ago      Up 26 seconds       0.0.0.0:32768-\n80/tcp   peaceful_varahamihira\n\n\n\n\nRenaming the container\n\n\nWe can rename the container by using following command  \n\n\ndocker rename 7d58ecc05754 vote\n\n\n\n\n[replace 7d58ecc05754 with the actual container id on your system ]\n\n\nWe have changed container's automatically generated name to \nvote\n. This new name can be of your choice. The point to understand is this command takes two arguments. The \nOld_name followed by New_name\n\nRun docker ps command to check the effect of changes  \n\n\ndocker ps\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                   NAMES\n7d58ecc05754        schoolofdevops/vote   \ngunicorn app:app -b\u2026\n   3 minutes ago       Up 3 minutes        0.0.0.0:32768-\n80/tcp   vote\n\n\n\n\nAs you can see here, the container is renamed to \nvote\n. This makes referencing container in cli very much easier.  \n\n\nReady to  vote ?\n\n\nLet's see what this \nvote\n application does by connecting to that application. For that we need,\n\n  * Host machine's IP\n\n  * Container's port which is mapped to a host's port\n\n\nLet's find out the port mapping of container to host. Docker provides subcommand called \nport\n which does this job  \n\n\ndocker port vote  \n\n\n\n\n[Output]  \n\n\n80/tcp -\n 0.0.0.0:32768\n\n\n\n\nSo whatever traffic the host gets in port \n2368\n will be mapped to container's port \n32768\n  \n\n\nLet's connect to http://IP_ADDRESS:PORT to see the actual application  \n\n\nFinding Everything about the running  container\n\n\nThis topic discusses about finding metadata of containers. These metadata include various parameters like,\n\n  * State of the container\n\n  * Mounts\n\n  * Configuration\n\n  * Network, etc.,  \n\n\nInspecting\n\n\nLets try this inspect subcommand in action  \n\n\ndocker inspect vote\n\n\n\n\nData output by above command contains detailed descriptino of the container an its properties. is represented in JSON format which makes filtering these results easier.  \n\n\nChecking the Stats\n\n\nStats command\n\n\nThis command returns a data stream of resource utilization used by containers. The flag \n--no-stream\n disables data stream and displays only first result  \n\n\ndocker stats --no-stream=true vote\n\ndocker stats\n\n\n\n\nTop command\n\n\nTo display the list of processes and the information about those processes that are running inside the container, we can use \ntop\n command\n\n\ndocker top vote\n\n\n\n\n[Output]  \n\n\nUID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD\nvagrant             6219                6211                0                   14:07               ?                   00:00:00            npm\nvagrant             6275                6219                0                   14:07               ?                   00:00:00            sh -c node index\nvagrant             6276                6275                0                   14:07               ?                   00:00:11            node index\n\n\n\n\n\nExamine Logs\n\n\nDocker \nlog\n command is to print the logs of the application inside the container. In our case we will see the log output of vote application  \n\n\ndocker logs vote\n\n\n\n\n[Output]  \n\n\n[2018-05-01 15:36:01 +0000] [1] [INFO] Starting gunicorn 19.6.0\n[2018-05-01 15:36:01 +0000] [1] [INFO] Listening at: http://0.0.0.0:80 (1)\n[2018-05-01 15:36:01 +0000] [1] [INFO] Using worker: sync\n[2018-05-01 15:36:01 +0000] [10] [INFO] Booting worker with pid: 10\n[2018-05-01 15:36:01 +0000] [11] [INFO] Booting worker with pid: 11\n[2018-05-01 15:36:01 +0000] [12] [INFO] Booting worker with pid: 12\n[2018-05-01 15:36:01 +0000] [15] [INFO] Booting worker with pid: 15```  \n\nIf you want to **follow** the log in real-time, use **-f** flag  \n\n\n\n\n\nTo follow the logs,\n\n\ndocker logs -f vote\n\n\n\n\nNow try to read the articles available in our blog and see the log output gets updated in real-time. Hit \nctrl+c\n to break the stream  \n\n\nStream events from the docker daemon\n\n\nDocker \nevents\n serves us with the stream of events or interactions that are happening with the docker daemon. This does not stream the log data of application inside the container. That is done by \ndocker logs\n command. Let us see how this command works\n\nOpen an another terminal. Let us call the old terminal as \nTerminal 1\n and the newer one as \nTerminal 2\n.\n\n\nFrom Terminal 1, execute \ndocker events\n. Now you are getting the data stream from docker daemon  \n\n\ndocker events\n\n\n\n\nTo understand how this command actually works, let us run a container from Terminal 2  \n\n\ndocker run -it alpine:3.4 sh  \n\n\n\n\nIf you see, in Terminal 1, the interaction with docker daemon, while running that container will be printed  \n\n\n[Output - \nTerminal 1\n]  \n\n\n2016-09-16T13:00:20.189028004Z container create 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin)\n2016-09-16T13:00:20.190190470Z container attach 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin)\n2016-09-16T13:00:20.257068692Z network connect c0237b5406920749b87460597b8935adf958bae1ce997afd827921a0dbc97cdc (container=816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923, name=bridge, type=bridge)\n2016-09-16T13:00:20.346533821Z container start 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin)\n2016-09-16T13:00:20.347811877Z container resize 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (height=41, image=alpine:3.4, name=tiny_franklin, width=126)\n\n\n\n\nTry to do various docker operations (start, stop, rm, etc.,) and see the output in Terminal 1  \n\n\nAttach to the container\n\n\nNormally, when we run a container, we use \n-d\n flag to run that container in detached mode. But sometimes you might require to make some changes inside that container. In those kind of situations, we can use \nattach\n command. This command attaches to the tty of docker container. So it will stream the output of the application. In our case, we will see the output of vote application  \n\n\ndocker attach vote\n\n\n\n\nHit our blogs url several times to see the output  \n\n\n[Output]  \n\n\nroot@swarm-03:~# docker attach vote\n[2018-05-01 15:44:49 +0000] [1] [INFO] Handling signal: winch\n\n\n\n\n\nYou can detach from the tty by pressing \nctrl-p + ctrl-q\n in sequence. If you haven't started your container with \n-it\n flag, then it is not possible to get your host's terminal back. In that case, If you haven't started the container with \n-it\n option, the only way you will be able to detach  from the container by using \nctrl-c\n, which kills the process, in turns the container itself.   \n\n\nIt is possible to override these keys too. For that we have to add --detach-keys flag to the command. To learn more, click on the following URL  \n\n\nhttps://docs.docker.com/engine/reference/commandline/attach/  \n\n\nCopying files between container and client host\n\n\nWe can copy files/directories form host to container and vice-versa  \n\nLet us create a file on the host  \n\n\ntouch testfile\n\n\n\n\nTo copy the testfile \nfrom host machine to ghsot contanier\n, try  \n\n\ndocker cp testfile vote:/opt  \n\n\n\n\nThis command will copy testfile to vote container's \n/opt\n directory  and will not give any output. To verify the file has been copies or not, let us log into container by running,  \n\n\ndocker exec -it vote bash\n\n\n\n\nChange directory into /opt and list the files  \n\n\ncd /opt  \nls\n\n\n\n\n[Output]  \n\n\ntestfile\n\n\n\n\nThere you can see that file has been successfully copied. Now exit the container  \n\n\nNow you may try to cp some files \nfrom the container to the host machine\n  \n\n\ndocker cp vote:/app  .  \nls  \n\n\n\n\nControlling Resources\n\n\nDocker provides us the granularity to control each container's \nresource utilization\n. We have several commands in the inventory to achieve this  \n\n\nPutting limits on Running Containers\n\n\nFirst, let us monitor the utilization\n\n\ndocker stats --no-stream=true\n\n\n\n\n[Example Output]  \n\n\ndocker stats --no-stream=true\nCONTAINER ID        NAME                                CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS\n7d58ecc05754        vote                                0.02%               56.5MiB / 1.955GiB    2.82%               648B / 0B           0B / 0B             0\n9fc1aec8cb6a        gallant_brattain                    0.00%               328KiB / 1.955GiB     0.02%               690B / 0B           0B / 0B             0\n08f0242aa61c        vote.2.qwxduexkwpmdnowouxjzwjwag    0.02%               56.33MiB / 1.955GiB   2.81%               1.94kB / 0B         0B / 0B             0\n8469b95efc81        redis.4.s5i3kid9yohpbim05bsw59sh2   0.12%               6.223MiB / 1.955GiB   0.31%               1.36kB / 0B         0B / 0B             0\nce823d38adaf        redis.1.qfx6geh6t9vuy8awq94u10m07   0.08%               6.227MiB / 1.955GiB   0.31%               7.25kB / 5.39kB     0B / 0B             0\n\n\n\n\nFor monitoring resources continuously,\n\n\ndocker stats --no-stream=true\n\n\n\n\nYou can see that \nMemory\n attribute has \n0\n as its value. 0 means unlimited usage of host's RAM. We can put a cap on that by using \nupdate\n command  \n\n\ndocker update --memory 400M --memory-swap -1 vote   \n\n\n\n\n[Output]  \n\n\nvote\n\n\n\n\nLet us check whether the change has taken effect or not  \n\n\ndocker inspect vote | grep -i memory\ndocker stat \n\n\n\n\n[Output]  \n\n\nMemory\n: 419430400,\n\nKernelMemory\n: 0,\n\nMemoryReservation\n: 0,\n\nMemorySwap\n: 0,\n\nMemorySwappiness\n: -1,\n\n\n\n\n\nAs you can see, the memory utilization of the container is changed from 0 (unlimited) to 400 mb  \n\n\nLimiting Resources while launching new containers\n\n\nThe following resources can be limited using the \nupdate\n command\n\n  * CPU\n  * Memory\n  * Disk IO\n  * Capabilities  \n\n\nOpen two terminals, lets call them T1, and T2\n\nIn T1, start monitoring the stats  \n\n\ndocker stats\n\n\n\n\n[Output]  \n\n\nCONTAINER           CPU %               MEM USAGE / LIMIT     MM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.16%               190.1 MiB / 400 MiB   47.51%              1.296 kB / 648 B    86.02 kB / 45.06 kB   0\nCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.01%               190.1 MiB / 400 MiB   47.51%              1.296 kB / 648 B    86.02 kB / 45.06 kB   0\n\n\n\n\nFrom T2, launch two containers with different CPU shares. Default CPU shares are set to 1024. This is a relative weight.  \n\n\ndocker run -d --name st-01  schoolofdevops/stresstest stress --cpu 1\n\ndocker run -d --name st-02 -c 512  schoolofdevops/stresstest stress --cpu 1\n\n\n\n\n\nWhen you launch the first container, it will use the full quota of CPU, i.e., 100%  \n\n\n[Output - \nAfter first container launch\n]  \n\n\nCONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.01%               190.1 MiB / 400 MiB     47.51%              1.944 kB / 648 B    86.02 kB / 45.06 kB   0\n764f158d6523        102.73%             2.945 MiB / 1.797 GiB   0.16%               648 B / 648 B       3.118 MB / 0 B        0\n\n\n\n\n[Output - \nAfter second container lauch\n]  \n\n\nCONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.00%               190.1 MiB / 400 MiB     47.51%              2.592 kB / 648 B    86.02 kB / 45.06 kB   0\n764f158d6523        66.97%              2.945 MiB / 1.797 GiB   0.16%               1.296 kB / 648 B    3.118 MB / 0 B        0\na13f98995ade        33.36%              2.945 MiB / 1.797 GiB   0.16%               648 B / 648 B       3.118 MB / 0 B        0\n\n\n\n\nObserve stats in T1\nLaunch a couple more nodes with different cpu shares, observe how T2 stats change  \n\n\ndocker run -d --name st-03 -c 512  schoolofdevops/stresstest stress --cpu 1\n\ndocker run -d --name st-04  schoolofdevops/stresstest stress --cpu 1\n\n\n\n\n\n[Output - \nAfter all containers are launched\n]  \n\n\nCONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.00%               190.1 MiB / 400 MiB     47.51%              3.888 kB / 648 B    86.02 kB / 45.06 kB   0\n764f158d6523        32.09%              2.945 MiB / 1.797 GiB   0.16%               2.592 kB / 648 B    3.118 MB / 0 B        0\na13f98995ade        16.02%              2.945 MiB / 1.797 GiB   0.16%               1.944 kB / 648 B    3.118 MB / 0 B        0\nf04e9ea5627c        16.37%              2.949 MiB / 1.797 GiB   0.16%               1.296 kB / 648 B    3.118 MB / 0 B        0\nabeab389a873        31.71%              2.949 MiB / 1.797 GiB   0.16%               648 B / 648 B       3.118 MB / 0 B        0\n\n\n\n\nClose the T2 terminal  \n\n\nExercises\n\n\nTry to these exercises, to get a better understanding\n\n  * Put a memory limit\n  * Set disk iops\n\n\nLaunching Containers with Elevated  Privileges\n\n\nWhen the operator executes docker run --privileged, Docker will enable to access to all devices on the host as well as set some configuration in AppArmor or SELinux to allow the container nearly all the same access to the host as processes running outside containers on the host.\n\n\nExample:\n\n\nRunning a sysdig container to monitor docker\n\n\nSysdig tool allows us to monitor the processes that are going on in the other containers. It is more like running a top command from one container on behalf of others.  \n\n\ndocker run -itd --name=sysdig --privileged=true \\\n           --volume=/var/run/docker.sock:/host/var/run/docker.sock \\\n           --volume=/dev:/host/dev \\\n           --volume=/proc:/host/proc:ro \\\n           --volume=/boot:/host/boot:ro \\\n           --volume=/lib/modules:/host/lib/modules:ro \\\n           --volume=/usr:/host/usr:ro \\\n           sysdig/sysdig:0.11.0 sysdig\n\n\n\n\n[Output]  \n\n\nUnable to find image 'sysdig/sysdig:0.11.0' locally\n0.11.0: Pulling from sysdig/sysdig\n\n0f409b0f5b3d: Pull complete\n64965da77fc6: Pull complete\n588eeb0d4c30: Pull complete\n9aa18e35b362: Pull complete\ncc036f2dca14: Pull complete\n33400f3af946: Pull complete\nb39ed90e36fd: Pull complete\n1fca16436380: Pull complete\nDigest: sha256:ee9d66a07308c5aef91f070cce5c9fb891e4fefb5da4d417e590662e34846664\nStatus: Downloaded newer image for sysdig/sysdig:0.11.0\n6ba17cf2af7b87621b3380517af45c5785dc8cda75111f0f8c36bb83e163a120\n\n\n\n\ndocker exec -it sysdig bash\ncsysdig\n\n\n\n\n[Output]  \n\n\n  \n\n\nAfter this, press f2 and select \ncontainers\n tab\n\nNow check what are the processes are running in other containers  \n\n\n  \n\n\nReferences\n\n\n[Resource Management in Docker by Marek Goldmann] (https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/)", 
            "title": "Container Operations"
        }, 
        {
            "location": "/chapter5-container_operations/#managing-containers-learning-about-common-container-operations", 
            "text": "In the previous chapter, we have learnt about container lifecycle management including how to create, launch, connect to, stop and remove containers. In this chapter, we are going to learn how to launch a container with a pre built app image and how to access the app with published ports. We will also learn about common container operations such as inspecting container information, checking logs and performance stats, renaming and updating the properties of a container, limiting resources etc.    As part of this lab, we are going to launch a python based webapp for a sample voting application.", 
            "title": "Managing Containers - Learning about Common Container Operations"
        }, 
        {
            "location": "/chapter5-container_operations/#launching-a-container-with-a-pre-built-app-image", 
            "text": "To launch vote container run the following command. Don't bother about the new flag  -P  now. We will explain about that flag later in this chapter    docker container run  -idt -P  schoolofdevops/vote  [Output]    Unable to find image 'schoolofdevops/vote:latest' locally\nlatest: Pulling from schoolofdevops/vote\nDigest: sha256:9195942ea654fa8d8aeb37900be5619215c08e7e1bef0b7dfe4c04a9cc20a8c2\nStatus: Downloaded newer image for schoolofdevops/vote:latest\n7d58ecc05754b5fd192c4ecceae334ac22565684c6923ea332bff5c88e3fca2b  Lets check the status of the container    docker ps -l  [Output]    CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                   NAMES\n7d58ecc05754        schoolofdevops/vote    gunicorn app:app -b\u2026    27 seconds ago      Up 26 seconds       0.0.0.0:32768- 80/tcp   peaceful_varahamihira", 
            "title": "Launching a container with a pre built app image"
        }, 
        {
            "location": "/chapter5-container_operations/#renaming-the-container", 
            "text": "We can rename the container by using following command    docker rename 7d58ecc05754 vote  [replace 7d58ecc05754 with the actual container id on your system ]  We have changed container's automatically generated name to  vote . This new name can be of your choice. The point to understand is this command takes two arguments. The  Old_name followed by New_name \nRun docker ps command to check the effect of changes    docker ps  [Output]    CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                   NAMES\n7d58ecc05754        schoolofdevops/vote    gunicorn app:app -b\u2026    3 minutes ago       Up 3 minutes        0.0.0.0:32768- 80/tcp   vote  As you can see here, the container is renamed to  vote . This makes referencing container in cli very much easier.", 
            "title": "Renaming the container"
        }, 
        {
            "location": "/chapter5-container_operations/#ready-to-vote", 
            "text": "Let's see what this  vote  application does by connecting to that application. For that we need, \n  * Host machine's IP \n  * Container's port which is mapped to a host's port  Let's find out the port mapping of container to host. Docker provides subcommand called  port  which does this job    docker port vote    [Output]    80/tcp -  0.0.0.0:32768  So whatever traffic the host gets in port  2368  will be mapped to container's port  32768     Let's connect to http://IP_ADDRESS:PORT to see the actual application", 
            "title": "Ready to  vote ?"
        }, 
        {
            "location": "/chapter5-container_operations/#finding-everything-about-the-running-container", 
            "text": "This topic discusses about finding metadata of containers. These metadata include various parameters like, \n  * State of the container \n  * Mounts \n  * Configuration \n  * Network, etc.,", 
            "title": "Finding Everything about the running  container"
        }, 
        {
            "location": "/chapter5-container_operations/#inspecting", 
            "text": "Lets try this inspect subcommand in action    docker inspect vote  Data output by above command contains detailed descriptino of the container an its properties. is represented in JSON format which makes filtering these results easier.", 
            "title": "Inspecting"
        }, 
        {
            "location": "/chapter5-container_operations/#checking-the-stats", 
            "text": "", 
            "title": "Checking the Stats"
        }, 
        {
            "location": "/chapter5-container_operations/#stats-command", 
            "text": "This command returns a data stream of resource utilization used by containers. The flag  --no-stream  disables data stream and displays only first result    docker stats --no-stream=true vote\n\ndocker stats", 
            "title": "Stats command"
        }, 
        {
            "location": "/chapter5-container_operations/#top-command", 
            "text": "To display the list of processes and the information about those processes that are running inside the container, we can use  top  command  docker top vote  [Output]    UID                 PID                 PPID                C                   STIME               TTY                 TIME                CMD\nvagrant             6219                6211                0                   14:07               ?                   00:00:00            npm\nvagrant             6275                6219                0                   14:07               ?                   00:00:00            sh -c node index\nvagrant             6276                6275                0                   14:07               ?                   00:00:11            node index", 
            "title": "Top command"
        }, 
        {
            "location": "/chapter5-container_operations/#examine-logs", 
            "text": "Docker  log  command is to print the logs of the application inside the container. In our case we will see the log output of vote application    docker logs vote  [Output]    [2018-05-01 15:36:01 +0000] [1] [INFO] Starting gunicorn 19.6.0\n[2018-05-01 15:36:01 +0000] [1] [INFO] Listening at: http://0.0.0.0:80 (1)\n[2018-05-01 15:36:01 +0000] [1] [INFO] Using worker: sync\n[2018-05-01 15:36:01 +0000] [10] [INFO] Booting worker with pid: 10\n[2018-05-01 15:36:01 +0000] [11] [INFO] Booting worker with pid: 11\n[2018-05-01 15:36:01 +0000] [12] [INFO] Booting worker with pid: 12\n[2018-05-01 15:36:01 +0000] [15] [INFO] Booting worker with pid: 15```  \n\nIf you want to **follow** the log in real-time, use **-f** flag    To follow the logs,  docker logs -f vote  Now try to read the articles available in our blog and see the log output gets updated in real-time. Hit  ctrl+c  to break the stream", 
            "title": "Examine Logs"
        }, 
        {
            "location": "/chapter5-container_operations/#stream-events-from-the-docker-daemon", 
            "text": "Docker  events  serves us with the stream of events or interactions that are happening with the docker daemon. This does not stream the log data of application inside the container. That is done by  docker logs  command. Let us see how this command works \nOpen an another terminal. Let us call the old terminal as  Terminal 1  and the newer one as  Terminal 2 .  From Terminal 1, execute  docker events . Now you are getting the data stream from docker daemon    docker events  To understand how this command actually works, let us run a container from Terminal 2    docker run -it alpine:3.4 sh    If you see, in Terminal 1, the interaction with docker daemon, while running that container will be printed    [Output -  Terminal 1 ]    2016-09-16T13:00:20.189028004Z container create 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin)\n2016-09-16T13:00:20.190190470Z container attach 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin)\n2016-09-16T13:00:20.257068692Z network connect c0237b5406920749b87460597b8935adf958bae1ce997afd827921a0dbc97cdc (container=816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923, name=bridge, type=bridge)\n2016-09-16T13:00:20.346533821Z container start 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (image=alpine:3.4, name=tiny_franklin)\n2016-09-16T13:00:20.347811877Z container resize 816fcc5e9c8dca13c76f3ff4546a7769bed497c4f4153b20ec34459c88f7b923 (height=41, image=alpine:3.4, name=tiny_franklin, width=126)  Try to do various docker operations (start, stop, rm, etc.,) and see the output in Terminal 1", 
            "title": "Stream events from the docker daemon"
        }, 
        {
            "location": "/chapter5-container_operations/#attach-to-the-container", 
            "text": "Normally, when we run a container, we use  -d  flag to run that container in detached mode. But sometimes you might require to make some changes inside that container. In those kind of situations, we can use  attach  command. This command attaches to the tty of docker container. So it will stream the output of the application. In our case, we will see the output of vote application    docker attach vote  Hit our blogs url several times to see the output    [Output]    root@swarm-03:~# docker attach vote\n[2018-05-01 15:44:49 +0000] [1] [INFO] Handling signal: winch  You can detach from the tty by pressing  ctrl-p + ctrl-q  in sequence. If you haven't started your container with  -it  flag, then it is not possible to get your host's terminal back. In that case, If you haven't started the container with  -it  option, the only way you will be able to detach  from the container by using  ctrl-c , which kills the process, in turns the container itself.     It is possible to override these keys too. For that we have to add --detach-keys flag to the command. To learn more, click on the following URL    https://docs.docker.com/engine/reference/commandline/attach/", 
            "title": "Attach to the container"
        }, 
        {
            "location": "/chapter5-container_operations/#copying-files-between-container-and-client-host", 
            "text": "We can copy files/directories form host to container and vice-versa   \nLet us create a file on the host    touch testfile  To copy the testfile  from host machine to ghsot contanier , try    docker cp testfile vote:/opt    This command will copy testfile to vote container's  /opt  directory  and will not give any output. To verify the file has been copies or not, let us log into container by running,    docker exec -it vote bash  Change directory into /opt and list the files    cd /opt  \nls  [Output]    testfile  There you can see that file has been successfully copied. Now exit the container    Now you may try to cp some files  from the container to the host machine     docker cp vote:/app  .  \nls", 
            "title": "Copying files between container and client host"
        }, 
        {
            "location": "/chapter5-container_operations/#controlling-resources", 
            "text": "Docker provides us the granularity to control each container's  resource utilization . We have several commands in the inventory to achieve this", 
            "title": "Controlling Resources"
        }, 
        {
            "location": "/chapter5-container_operations/#putting-limits-on-running-containers", 
            "text": "First, let us monitor the utilization  docker stats --no-stream=true  [Example Output]    docker stats --no-stream=true\nCONTAINER ID        NAME                                CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O           PIDS\n7d58ecc05754        vote                                0.02%               56.5MiB / 1.955GiB    2.82%               648B / 0B           0B / 0B             0\n9fc1aec8cb6a        gallant_brattain                    0.00%               328KiB / 1.955GiB     0.02%               690B / 0B           0B / 0B             0\n08f0242aa61c        vote.2.qwxduexkwpmdnowouxjzwjwag    0.02%               56.33MiB / 1.955GiB   2.81%               1.94kB / 0B         0B / 0B             0\n8469b95efc81        redis.4.s5i3kid9yohpbim05bsw59sh2   0.12%               6.223MiB / 1.955GiB   0.31%               1.36kB / 0B         0B / 0B             0\nce823d38adaf        redis.1.qfx6geh6t9vuy8awq94u10m07   0.08%               6.227MiB / 1.955GiB   0.31%               7.25kB / 5.39kB     0B / 0B             0  For monitoring resources continuously,  docker stats --no-stream=true  You can see that  Memory  attribute has  0  as its value. 0 means unlimited usage of host's RAM. We can put a cap on that by using  update  command    docker update --memory 400M --memory-swap -1 vote     [Output]    vote  Let us check whether the change has taken effect or not    docker inspect vote | grep -i memory\ndocker stat   [Output]    Memory : 419430400, KernelMemory : 0, MemoryReservation : 0, MemorySwap : 0, MemorySwappiness : -1,  As you can see, the memory utilization of the container is changed from 0 (unlimited) to 400 mb", 
            "title": "Putting limits on Running Containers"
        }, 
        {
            "location": "/chapter5-container_operations/#limiting-resources-while-launching-new-containers", 
            "text": "The following resources can be limited using the  update  command \n  * CPU\n  * Memory\n  * Disk IO\n  * Capabilities    Open two terminals, lets call them T1, and T2 \nIn T1, start monitoring the stats    docker stats  [Output]    CONTAINER           CPU %               MEM USAGE / LIMIT     MM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.16%               190.1 MiB / 400 MiB   47.51%              1.296 kB / 648 B    86.02 kB / 45.06 kB   0\nCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.01%               190.1 MiB / 400 MiB   47.51%              1.296 kB / 648 B    86.02 kB / 45.06 kB   0  From T2, launch two containers with different CPU shares. Default CPU shares are set to 1024. This is a relative weight.    docker run -d --name st-01  schoolofdevops/stresstest stress --cpu 1\n\ndocker run -d --name st-02 -c 512  schoolofdevops/stresstest stress --cpu 1  When you launch the first container, it will use the full quota of CPU, i.e., 100%    [Output -  After first container launch ]    CONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.01%               190.1 MiB / 400 MiB     47.51%              1.944 kB / 648 B    86.02 kB / 45.06 kB   0\n764f158d6523        102.73%             2.945 MiB / 1.797 GiB   0.16%               648 B / 648 B       3.118 MB / 0 B        0  [Output -  After second container lauch ]    CONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.00%               190.1 MiB / 400 MiB     47.51%              2.592 kB / 648 B    86.02 kB / 45.06 kB   0\n764f158d6523        66.97%              2.945 MiB / 1.797 GiB   0.16%               1.296 kB / 648 B    3.118 MB / 0 B        0\na13f98995ade        33.36%              2.945 MiB / 1.797 GiB   0.16%               648 B / 648 B       3.118 MB / 0 B        0  Observe stats in T1\nLaunch a couple more nodes with different cpu shares, observe how T2 stats change    docker run -d --name st-03 -c 512  schoolofdevops/stresstest stress --cpu 1\n\ndocker run -d --name st-04  schoolofdevops/stresstest stress --cpu 1  [Output -  After all containers are launched ]    CONTAINER           CPU %               MEM USAGE / LIMIT       MEM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.00%               190.1 MiB / 400 MiB     47.51%              3.888 kB / 648 B    86.02 kB / 45.06 kB   0\n764f158d6523        32.09%              2.945 MiB / 1.797 GiB   0.16%               2.592 kB / 648 B    3.118 MB / 0 B        0\na13f98995ade        16.02%              2.945 MiB / 1.797 GiB   0.16%               1.944 kB / 648 B    3.118 MB / 0 B        0\nf04e9ea5627c        16.37%              2.949 MiB / 1.797 GiB   0.16%               1.296 kB / 648 B    3.118 MB / 0 B        0\nabeab389a873        31.71%              2.949 MiB / 1.797 GiB   0.16%               648 B / 648 B       3.118 MB / 0 B        0  Close the T2 terminal", 
            "title": "Limiting Resources while launching new containers"
        }, 
        {
            "location": "/chapter5-container_operations/#exercises", 
            "text": "Try to these exercises, to get a better understanding \n  * Put a memory limit\n  * Set disk iops", 
            "title": "Exercises"
        }, 
        {
            "location": "/chapter5-container_operations/#launching-containers-with-elevated-privileges", 
            "text": "When the operator executes docker run --privileged, Docker will enable to access to all devices on the host as well as set some configuration in AppArmor or SELinux to allow the container nearly all the same access to the host as processes running outside containers on the host.", 
            "title": "Launching Containers with Elevated  Privileges"
        }, 
        {
            "location": "/chapter5-container_operations/#example", 
            "text": "", 
            "title": "Example:"
        }, 
        {
            "location": "/chapter5-container_operations/#running-a-sysdig-container-to-monitor-docker", 
            "text": "Sysdig tool allows us to monitor the processes that are going on in the other containers. It is more like running a top command from one container on behalf of others.    docker run -itd --name=sysdig --privileged=true \\\n           --volume=/var/run/docker.sock:/host/var/run/docker.sock \\\n           --volume=/dev:/host/dev \\\n           --volume=/proc:/host/proc:ro \\\n           --volume=/boot:/host/boot:ro \\\n           --volume=/lib/modules:/host/lib/modules:ro \\\n           --volume=/usr:/host/usr:ro \\\n           sysdig/sysdig:0.11.0 sysdig  [Output]    Unable to find image 'sysdig/sysdig:0.11.0' locally\n0.11.0: Pulling from sysdig/sysdig\n\n0f409b0f5b3d: Pull complete\n64965da77fc6: Pull complete\n588eeb0d4c30: Pull complete\n9aa18e35b362: Pull complete\ncc036f2dca14: Pull complete\n33400f3af946: Pull complete\nb39ed90e36fd: Pull complete\n1fca16436380: Pull complete\nDigest: sha256:ee9d66a07308c5aef91f070cce5c9fb891e4fefb5da4d417e590662e34846664\nStatus: Downloaded newer image for sysdig/sysdig:0.11.0\n6ba17cf2af7b87621b3380517af45c5785dc8cda75111f0f8c36bb83e163a120  docker exec -it sysdig bash\ncsysdig  [Output]        After this, press f2 and select  containers  tab \nNow check what are the processes are running in other containers", 
            "title": "Running a sysdig container to monitor docker"
        }, 
        {
            "location": "/chapter5-container_operations/#references", 
            "text": "[Resource Management in Docker by Marek Goldmann] (https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/)", 
            "title": "References"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/", 
            "text": "Dockerizing your Applications : Building Images and Working with Registries\n\n\nIn the previous session, we have learnt about various container operations such as running containers from\npre built images, port mapping, inspecting and updating containers, limiting resources etc., In this\nchapter, we are going to learn about how to build containers for your individual applications, as well\nas how to work with docker hub registry to host and distribute the images.\n\n\nLab: Registering with the DockerHub\n\n\nSince we are going to start working with the registry, build and push images to it later, its essential to have our own account on the registry. For the purpose of this tutorial, we are going to use the hosted registry i.e. Dockerhub.\n\n\nSteps to create Dockerhub account\n\n\nStep 1:\n\n\nVisit the following link and sign up with your email id\n\n\nhttps://hub.docker.com/\n\n\n\n\nStep 2:\n\n\nCheck your email inbox and check the activation email sent by docker team\n\n\nStep 3:\n\n\nAfter clicking on the activation link, you will be redirected to a log in page. Enter your credentials and log in\n\n\n\n\nYou will be launched to Dockerhub main page. Now the registration process is complete and you have account in Dockerhub!\n\n\n\n\nLab: Building Docker Images - A manual approach\n\n\nBefore we start building automated images, we are going to create a docker image by hand. We have already used the pre built image from the registry in the last session. In this session, however, we are going to create our own image with ghost installed. Since Ghost is a node.js based application, we will base our work on existing official image for \nnode\n\n\nClone Repository for Java worker app\n\n\ngit clone https://github.com/schoolofdevops/voting-app-worker.git\n\n\n\n\nLaunch a intermediate container to install worker app\n\n\nCreate a Container with  \nschoolofdevops/voteapp-mvn:v1\n image\n\n\ndocker run -idt --name interim schoolofdevops/voteapp-mvn  sh\n\n\n\n\n\nCopy over the Source Code\n\n\ncd voting-app-worker\ndocker container cp .  interim:/code\n\n\n\n\n\nConnect to container to compile and package the code\n\n\ndocker exec -it interim sh\n\nmvn package\n\n\n\n\n\nVerify jarfile has been built\n\n\nls target/\n\n\n\n\n\nCommit  container to an image\n\n\n\n\nExit from the container shell\n\n\nNote container ID\n\n\n\n\n\n  docker container commit interim  \ndocker_hub_id\n/worker:v1\n\n\n\n\n\nTest before pushing  by launching container with the packaged app\n\n\n  docker run --rm -it  \ndocker hub user id \n/worker:v1 java -jar target/worker-jar-with-dependencies.jar\n\n\n\n\nPush Image to registry\n\n\nBefore you push the image, you need to be logged in to the registry, with the docker hub id created earlier. Login using the following command,\n\n\ndocker login\n\n\n\n\nTo push the image, first list it,\n\n\ndocker image ls\n\n\n\n\n[Sample Output]\n\n\nREPOSITORY                   TAG                 IMAGE ID            CREATED             SIZE\ninitcron/worker         v2              90cbeb6539df        18 minutes ago      194MB\ninitcron/worker         v1              c0199f782489        34 minutes ago      189MB\n\n\n\n\n\nTo push the image,\n\n\ndocker push \ndockrhub user id\n/worker:v1\n\n\n\n\nLab: Building Images with Dockerfile\n\n\nNow, lets build the same image, this time with Dockerfile. To do this, create a file by name \nDockerfile\n in the root of the source code.\n\n\nfile: Dockerfile\n\n\nFROM schoolofdevops/maven\n\nWORKDIR /app\n\nCOPY .  .\n\nRUN mvn package \n \\\n    mv target/worker-jar-with-dependencies.jar /run/worker.jar \n \\\n    rm -rf /app/*\n\nCMD java  -jar /run/worker.jar\n\n\n\n\n\nLets now build the image\n\n\ndocker image build -t \ndockrhub user id\n/worker:v2 .\n\ndocker image ls\n\n\n\n\n\nTry building again,\n\n\ndocker image build -t \ndockrhub user id\n/worker:v2 .\n\n\n\n\n\nThis time, it does not build everything, but uses cache.\n\n\nTesting the image\n\n\ndocker container run --rm -it  \ndockrhub user id\n/worker:v2\n\n\n\n\n\nTag the image as latest,\n\n\ndocker image tag  \ndockrhub user id\n/worker:v2  \ndockrhub user id\n/worker:latest\n\ndocker image ls\n\n\n\n\n\nFinally, publish it to the registry,\n\n\ndocker image push \ndockrhub user id\n/worker:latest\n\ndocker image push \ndockrhub user id\n/worker\n\n\n\n\n\nReferences\n\n\nBuilding Base Images: https://docs.docker.com/develop/develop-images/baseimages/", 
            "title": "Dockerizing your Apps"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#dockerizing-your-applications-building-images-and-working-with-registries", 
            "text": "In the previous session, we have learnt about various container operations such as running containers from\npre built images, port mapping, inspecting and updating containers, limiting resources etc., In this\nchapter, we are going to learn about how to build containers for your individual applications, as well\nas how to work with docker hub registry to host and distribute the images.", 
            "title": "Dockerizing your Applications : Building Images and Working with Registries"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#lab-registering-with-the-dockerhub", 
            "text": "Since we are going to start working with the registry, build and push images to it later, its essential to have our own account on the registry. For the purpose of this tutorial, we are going to use the hosted registry i.e. Dockerhub.  Steps to create Dockerhub account", 
            "title": "Lab: Registering with the DockerHub"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#step-1", 
            "text": "Visit the following link and sign up with your email id  https://hub.docker.com/", 
            "title": "Step 1:"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#step-2", 
            "text": "Check your email inbox and check the activation email sent by docker team", 
            "title": "Step 2:"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#step-3", 
            "text": "After clicking on the activation link, you will be redirected to a log in page. Enter your credentials and log in   You will be launched to Dockerhub main page. Now the registration process is complete and you have account in Dockerhub!", 
            "title": "Step 3:"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#lab-building-docker-images-a-manual-approach", 
            "text": "Before we start building automated images, we are going to create a docker image by hand. We have already used the pre built image from the registry in the last session. In this session, however, we are going to create our own image with ghost installed. Since Ghost is a node.js based application, we will base our work on existing official image for  node", 
            "title": "Lab: Building Docker Images - A manual approach"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#clone-repository-for-java-worker-app", 
            "text": "git clone https://github.com/schoolofdevops/voting-app-worker.git", 
            "title": "Clone Repository for Java worker app"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#launch-a-intermediate-container-to-install-worker-app", 
            "text": "Create a Container with   schoolofdevops/voteapp-mvn:v1  image  docker run -idt --name interim schoolofdevops/voteapp-mvn  sh", 
            "title": "Launch a intermediate container to install worker app"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#copy-over-the-source-code", 
            "text": "cd voting-app-worker\ndocker container cp .  interim:/code  Connect to container to compile and package the code  docker exec -it interim sh\n\nmvn package", 
            "title": "Copy over the Source Code"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#verify-jarfile-has-been-built", 
            "text": "ls target/  Commit  container to an image   Exit from the container shell  Note container ID   \n  docker container commit interim   docker_hub_id /worker:v1  Test before pushing  by launching container with the packaged app    docker run --rm -it   docker hub user id  /worker:v1 java -jar target/worker-jar-with-dependencies.jar", 
            "title": "Verify jarfile has been built"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#push-image-to-registry", 
            "text": "Before you push the image, you need to be logged in to the registry, with the docker hub id created earlier. Login using the following command,  docker login  To push the image, first list it,  docker image ls  [Sample Output]  REPOSITORY                   TAG                 IMAGE ID            CREATED             SIZE\ninitcron/worker         v2              90cbeb6539df        18 minutes ago      194MB\ninitcron/worker         v1              c0199f782489        34 minutes ago      189MB  To push the image,  docker push  dockrhub user id /worker:v1", 
            "title": "Push Image to registry"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#lab-building-images-with-dockerfile", 
            "text": "Now, lets build the same image, this time with Dockerfile. To do this, create a file by name  Dockerfile  in the root of the source code.  file: Dockerfile  FROM schoolofdevops/maven\n\nWORKDIR /app\n\nCOPY .  .\n\nRUN mvn package   \\\n    mv target/worker-jar-with-dependencies.jar /run/worker.jar   \\\n    rm -rf /app/*\n\nCMD java  -jar /run/worker.jar  Lets now build the image  docker image build -t  dockrhub user id /worker:v2 .\n\ndocker image ls  Try building again,  docker image build -t  dockrhub user id /worker:v2 .  This time, it does not build everything, but uses cache.  Testing the image  docker container run --rm -it   dockrhub user id /worker:v2  Tag the image as latest,  docker image tag   dockrhub user id /worker:v2   dockrhub user id /worker:latest\n\ndocker image ls  Finally, publish it to the registry,  docker image push  dockrhub user id /worker:latest\n\ndocker image push  dockrhub user id /worker", 
            "title": "Lab: Building Images with Dockerfile"
        }, 
        {
            "location": "/chapter6-dockerizing-your-apps/#references", 
            "text": "Building Base Images: https://docs.docker.com/develop/develop-images/baseimages/", 
            "title": "References"
        }, 
        {
            "location": "/chapter7-linking_containers/", 
            "text": "Building Application Stacks - Defining and Running Multi Container Apps\n\n\nLab: Creating a Docker Compose Stack for the Vote Application\n\n\nLets first launch redis and vote independently  and see if they automatically connect.\n\n\ndocker container run -idt --name redis redis:alpine\n\ndocker container  run -idt  --name vote -P  schoolofdevops/vote\n\n\n\n\n\nTry registering a vote with the voteapp UI.  Does it work?\n\n\nLinking services\n\n\nRemove vote container created above if any, and re launch it with the link.\n\n\ndocker container rm -f vote\n\ndocker container  run -idt  --name vote --link redis:redis -P  schoolofdevops/vote\n\n\n\n\nLaunch worker app as well with the link\n\n\n\ndocker container  run -idt  --name worker --link redis:redis -P  schoolofdevops/vote-worker\n\n\ndocker logs worker\n\n\n\n\nLaunching inter linked services with Compose spec\n\n\nLets now create a docker-compose spec and launch the services with docker-compose utility.\n\n\nCreate a directory to keep the compose files. Lets say \nstack\n\n\nmkdir stack\ncd stack\n\n\n\n\nfile: docker-compose.yml\n\n\nvote:\n  image: schoolofdevops/vote\n  links:\n    - redis:redis\n  ports:\n    - 80   \n\nredis:\n  image: redis:alpine\n\nworker:\n  image: schoolofdevops/vote-worker\n  links:\n    - redis:redis\n\n\n\n\n\nSyntax check\n\n\ndocker-compose config\n\n\n\n\nNow launch it with\n\n\ndocker-compose up -d\n\ndocker-compose ps\n\n\n\n\n\nfile: docker-compose-v3.yml\n\n\nversion: \n3\n\n\nnetworks:\n  vote:\n    driver: bridge\n\nservices:\n  vote:\n    image: schoolofdevops/vote\n    ports:\n      - 80\n    networks:\n      - vote\n    depends_on:\n      - redis\n\n  redis:\n    image: redis:alpine\n    networks:\n      - vote\n\n  worker:\n    image: schoolofdevops/vote-worker\n    networks:\n      - vote\n    depends_on:\n      - redis\n\n\n\n\nLaunch the new stack with,\n\n\ndocker-compose -f docker-compose-v3.yml up -d\n\n\ndocker-compose -f docker-compose-v3.yml ps\n\n\ndocker-compose -f docker-compose-v3.yml down", 
            "title": "Connecting apps with Docker Compose"
        }, 
        {
            "location": "/chapter7-linking_containers/#building-application-stacks-defining-and-running-multi-container-apps", 
            "text": "", 
            "title": "Building Application Stacks - Defining and Running Multi Container Apps"
        }, 
        {
            "location": "/chapter7-linking_containers/#lab-creating-a-docker-compose-stack-for-the-vote-application", 
            "text": "Lets first launch redis and vote independently  and see if they automatically connect.  docker container run -idt --name redis redis:alpine\n\ndocker container  run -idt  --name vote -P  schoolofdevops/vote  Try registering a vote with the voteapp UI.  Does it work?", 
            "title": "Lab: Creating a Docker Compose Stack for the Vote Application"
        }, 
        {
            "location": "/chapter7-linking_containers/#linking-services", 
            "text": "Remove vote container created above if any, and re launch it with the link.  docker container rm -f vote\n\ndocker container  run -idt  --name vote --link redis:redis -P  schoolofdevops/vote  Launch worker app as well with the link  \ndocker container  run -idt  --name worker --link redis:redis -P  schoolofdevops/vote-worker\n\n\ndocker logs worker", 
            "title": "Linking services"
        }, 
        {
            "location": "/chapter7-linking_containers/#launching-inter-linked-services-with-compose-spec", 
            "text": "Lets now create a docker-compose spec and launch the services with docker-compose utility.  Create a directory to keep the compose files. Lets say  stack  mkdir stack\ncd stack  file: docker-compose.yml  vote:\n  image: schoolofdevops/vote\n  links:\n    - redis:redis\n  ports:\n    - 80   \n\nredis:\n  image: redis:alpine\n\nworker:\n  image: schoolofdevops/vote-worker\n  links:\n    - redis:redis  Syntax check  docker-compose config  Now launch it with  docker-compose up -d\n\ndocker-compose ps  file: docker-compose-v3.yml  version:  3 \n\nnetworks:\n  vote:\n    driver: bridge\n\nservices:\n  vote:\n    image: schoolofdevops/vote\n    ports:\n      - 80\n    networks:\n      - vote\n    depends_on:\n      - redis\n\n  redis:\n    image: redis:alpine\n    networks:\n      - vote\n\n  worker:\n    image: schoolofdevops/vote-worker\n    networks:\n      - vote\n    depends_on:\n      - redis  Launch the new stack with,  docker-compose -f docker-compose-v3.yml up -d\n\n\ndocker-compose -f docker-compose-v3.yml ps\n\n\ndocker-compose -f docker-compose-v3.yml down", 
            "title": "Launching inter linked services with Compose spec"
        }, 
        {
            "location": "/chapterx-docker-networking/", 
            "text": "Lab: Docker Networking\n\n\nHost Networking\n\n\n\n\nbridge\n\n\nhost\n\n\npeer\n\n\nnone\n\n\n\n\nExamine the existing network\n\n\ndocker network ls\n\nNETWORK ID          NAME                DRIVER              SCOPE\nb3d405dd37e4        bridge              bridge              local\n7527c821537c        host                host                local\n773bea4ca095        none                null                local\n\n\n\n\nCreating new network\n\n\ndocker network create -d bridge mynet\n\n\n\n\nvalidate\n\n\ndocker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nb3d405dd37e4        bridge              bridge              local\n7527c821537c        host                host                local\n4e0d9b1a39f8        mynet               bridge              local\n773bea4ca095        none                null                local\n\n\n\n\ndocker network inspect mynet\n\n\n[\n   {\n       \nName\n: \nmynet\n,\n       \nId\n: \n4e0d9b1a39f859af4811986534c91527146bc9d2ce178e5de02473c0f8ce62d5\n,\n       \nCreated\n: \n2018-05-03T04:44:19.187296148Z\n,\n       \nScope\n: \nlocal\n,\n       \nDriver\n: \nbridge\n,\n       \nEnableIPv6\n: false,\n       \nIPAM\n: {\n           \nDriver\n: \ndefault\n,\n           \nOptions\n: {},\n           \nConfig\n: [\n               {\n                   \nSubnet\n: \n172.18.0.0/16\n,\n                   \nGateway\n: \n172.18.0.1\n\n               }\n           ]\n       },\n       \nInternal\n: false,\n       \nAttachable\n: false,\n       \nIngress\n: false,\n       \nConfigFrom\n: {\n           \nNetwork\n: \n\n       },\n       \nConfigOnly\n: false,\n       \nContainers\n: {},\n       \nOptions\n: {},\n       \nLabels\n: {}\n   }\n]\n\n\n\n\nLaunching containers in different bridges\n\n\nLaunch two containers \nnt01\n and \nnt02\n in \ndefault\n bridge network\n\n\ndocker container run -idt --name nt01 alpine sh\ndocker container run -idt --name nt02 alpine sh\n\n\n\n\nLaunch two containers \nnt03\n and \nnt04\n in \nmynet\n bridge network\n\n\ndocker container run -idt --name nt03 --net mynet alpine sh\ndocker container run -idt --name nt04 --net mynet alpine sh\n\n\n\n\nNow, lets examine if they can interconnect,\n\n\n\ndocker exec nt01 ifconfig eth0\ndocker exec nt02 ifconfig eth0\ndocker exec nt03 ifconfig eth0\ndocker exec nt04 ifconfig eth0\n\n\n\n\n\nThis is what I see\n\n\nnt01 :  172.17.0.18\n\n\nnt02 :  172.17.0.19\n\n\nnt03 :  172.18.0.2\n\n\nnt04 :  172.18.0.3\n\n\nCreate a table with the ips on your host.  Once you do that,\n\n\nTry to,\n  * ping from \nnt01\n to \nnt02\n\n  * ping from \nnt01\n to \nnt03\n\n  * ping from \nnt03\n to \nnt04\n\n  * ping from \nnt03\n to \nnt02\n\n\ne.g.\n\n\n[replace ip addresses as per your setup]\n\n\ndocker exec nt01  ping 172.17.0.19\n\ndocker exec nt01  ping 172.18.0.2\n\ndocker exec nt03  ping 172.17.0.19\n\ndocker exec nt03  ping 172.18.0.2\n\n\n\n\n\n\nClearly, these two are two differnt subnets/networks even though running on the same host. \nnt01\n and \nnt02\n can connect with each other, whereas \nnt03\n  and \nnt04\n can connect. But connection between containers attached to two different subnets is not possible.\n\n\nUsing None Network Driver\n\n\ndocker container run -idt --name nt05 --net none alpine sh\n\ndocker exec -it nt05 sh\n\nifconfig\n\n\n\n\nUsing Host Network Driver\n\n\ndocker container run -idt --name nt05 --net host  alpine sh\n\ndocker exec -it nt05 sh\n\nifconfig", 
            "title": "Docker Single Host Networking"
        }, 
        {
            "location": "/chapterx-docker-networking/#lab-docker-networking", 
            "text": "", 
            "title": "Lab: Docker Networking"
        }, 
        {
            "location": "/chapterx-docker-networking/#host-networking", 
            "text": "bridge  host  peer  none   Examine the existing network  docker network ls\n\nNETWORK ID          NAME                DRIVER              SCOPE\nb3d405dd37e4        bridge              bridge              local\n7527c821537c        host                host                local\n773bea4ca095        none                null                local  Creating new network  docker network create -d bridge mynet  validate  docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nb3d405dd37e4        bridge              bridge              local\n7527c821537c        host                host                local\n4e0d9b1a39f8        mynet               bridge              local\n773bea4ca095        none                null                local  docker network inspect mynet\n\n\n[\n   {\n        Name :  mynet ,\n        Id :  4e0d9b1a39f859af4811986534c91527146bc9d2ce178e5de02473c0f8ce62d5 ,\n        Created :  2018-05-03T04:44:19.187296148Z ,\n        Scope :  local ,\n        Driver :  bridge ,\n        EnableIPv6 : false,\n        IPAM : {\n            Driver :  default ,\n            Options : {},\n            Config : [\n               {\n                    Subnet :  172.18.0.0/16 ,\n                    Gateway :  172.18.0.1 \n               }\n           ]\n       },\n        Internal : false,\n        Attachable : false,\n        Ingress : false,\n        ConfigFrom : {\n            Network :  \n       },\n        ConfigOnly : false,\n        Containers : {},\n        Options : {},\n        Labels : {}\n   }\n]", 
            "title": "Host Networking"
        }, 
        {
            "location": "/chapterx-docker-networking/#launching-containers-in-different-bridges", 
            "text": "Launch two containers  nt01  and  nt02  in  default  bridge network  docker container run -idt --name nt01 alpine sh\ndocker container run -idt --name nt02 alpine sh  Launch two containers  nt03  and  nt04  in  mynet  bridge network  docker container run -idt --name nt03 --net mynet alpine sh\ndocker container run -idt --name nt04 --net mynet alpine sh  Now, lets examine if they can interconnect,  \ndocker exec nt01 ifconfig eth0\ndocker exec nt02 ifconfig eth0\ndocker exec nt03 ifconfig eth0\ndocker exec nt04 ifconfig eth0  This is what I see  nt01 :  172.17.0.18  nt02 :  172.17.0.19  nt03 :  172.18.0.2  nt04 :  172.18.0.3  Create a table with the ips on your host.  Once you do that,  Try to,\n  * ping from  nt01  to  nt02 \n  * ping from  nt01  to  nt03 \n  * ping from  nt03  to  nt04 \n  * ping from  nt03  to  nt02  e.g.  [replace ip addresses as per your setup]  docker exec nt01  ping 172.17.0.19\n\ndocker exec nt01  ping 172.18.0.2\n\ndocker exec nt03  ping 172.17.0.19\n\ndocker exec nt03  ping 172.18.0.2  Clearly, these two are two differnt subnets/networks even though running on the same host.  nt01  and  nt02  can connect with each other, whereas  nt03   and  nt04  can connect. But connection between containers attached to two different subnets is not possible.", 
            "title": "Launching containers in different bridges"
        }, 
        {
            "location": "/chapterx-docker-networking/#using-none-network-driver", 
            "text": "docker container run -idt --name nt05 --net none alpine sh\n\ndocker exec -it nt05 sh\n\nifconfig", 
            "title": "Using None Network Driver"
        }, 
        {
            "location": "/chapterx-docker-networking/#using-host-network-driver", 
            "text": "docker container run -idt --name nt05 --net host  alpine sh\n\ndocker exec -it nt05 sh\n\nifconfig", 
            "title": "Using Host Network Driver"
        }, 
        {
            "location": "/persistent-volumes/", 
            "text": "Lab: Persistent Volumes with Docker\n\n\nTypes of volumes\n\n\n\n\nautomatic volumes\n\n\nnamed volumes\n\n\nvolume binding\n\n\n\n\nAutomatic Volumes\n\n\ndocker container run  -idt --name vt01 -v /var/lib/mysql  alpine sh\ndocker inspect vt01 | grep -i mounts -A 10\n\n\n\n\nNamed volumes\n\n\ndocker container run  -idt --name vt02 -v db-data:/var/lib/mysql  alpine sh\ndocker inspect vt02 | grep -i mounts -A 10\n\n\n\n\nVolume binding\n\n\nmkdir /root/sysfoo\ndocker container run  -idt --name vt03 -v /root/sysfoo:/var/lib/mysql  alpine sh\ndocker inspect vt03 | grep -i mounts -A 10\n\n\n\n\nSharing files between host and the container\n\n\nls /root/sysfoo/\ntouch /root/sysfoo/file1\ndocker exec -it vt03 sh\nls sysfoo/", 
            "title": "Docker Data Persistence"
        }, 
        {
            "location": "/persistent-volumes/#lab-persistent-volumes-with-docker", 
            "text": "", 
            "title": "Lab: Persistent Volumes with Docker"
        }, 
        {
            "location": "/persistent-volumes/#types-of-volumes", 
            "text": "automatic volumes  named volumes  volume binding   Automatic Volumes  docker container run  -idt --name vt01 -v /var/lib/mysql  alpine sh\ndocker inspect vt01 | grep -i mounts -A 10  Named volumes  docker container run  -idt --name vt02 -v db-data:/var/lib/mysql  alpine sh\ndocker inspect vt02 | grep -i mounts -A 10  Volume binding  mkdir /root/sysfoo\ndocker container run  -idt --name vt03 -v /root/sysfoo:/var/lib/mysql  alpine sh\ndocker inspect vt03 | grep -i mounts -A 10  Sharing files between host and the container  ls /root/sysfoo/\ntouch /root/sysfoo/file1\ndocker exec -it vt03 sh\nls sysfoo/", 
            "title": "Types of volumes"
        }, 
        {
            "location": "/swarm-quickdive/", 
            "text": "Lab: Docker SWARM Quick Dive\n\n\nCreate a 5 nodes (3 masters, 2nodes) swarm cluster using http://play-with-docker.com\n\n\nLaunch a Visualizer on Master (SWARM Manager)\n\n\ndocker run -itd -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock schoolofdevops/visualizer\n\n\n\n\n\nDeploying Service with swarm - The imperative way\n\n\ndocker service create --name vote schoolofdevops/vote\n\n\n\n\ndocker service ls\ndocker service inspect\n\n\n\n\n\ndocker service  update --publish-add 80:80 vote\n\n\n\n\nTry accessing port 80 on any of the nodes in the swarm cluster to validate.\n\n\nScaling a service\n\n\ndocker service scale vote=4\ndocker service  ls\ndocker service scale vote=2\n\n\n\n\nCleaning Up\n\n\ndocker service rm vote\n\n\n\n\nOrchestrating Applications with Stack Deploy\n\n\nfile: stack.yml\n\n\nversion: \n3\n\n\nnetworks:\n  nw01:\n    driver: overlay\n\nvolumes:\n  db-data:\n\nservices:\n  vote:\n    image: schoolofdevops/vote:v1\n    ports:\n      - 80\n    networks:\n      - nw01\n    depends_on:\n      - redis\n    deploy:\n      replicas: 8\n      update_config:\n        parallelism: 2\n        delay: 20s\n      restart_policy:\n        condition: on-failure  \n\n  redis:\n    image: redis:alpine\n    networks:\n      - nw01\n\n  worker:\n    image: schoolofdevops/vote-worker\n    networks:\n      - nw01\n    depends_on:\n      - redis\n      - db\n\n  db:\n    image: postgres:9.4\n    networks:\n      - nw01\n    volumes:\n      - db-data:/var/lib/postgresql/data\n\n  result:\n    image: schoolofdevops/vote-result\n    ports:\n      - 5001:80\n    networks:\n      - nw01\n    depends_on:\n      - db\n\n\n\n\n\nYou could also copy the above file using the followinng command,\n\n\nwget -chttps://gist.githubusercontent.com/initcron/8a5ebd534df74ab2a83e96218b56137d/raw/9e748637aed121b67ceddeca8678750596c81ab7/stack.yml\n\n\n\n\nDeploy a stack\n\n\ndocker stack deploy --compose-file stack.yml instavote\n\n\n\n\n\nValidate\n\n\ndocker stack ls\n\ndocker stack services instavote\n\ndocker service ls\n\ndocker service scale instavote_vote=4\n\n\n\n\nDeploying a new version\n\n\nUpdate stack.yml with the new version of the image\n\n\n....\nservices:\n  vote:\n    image: schoolofdevops/vote:v2\n  .....\n\n    deploy:\n      replicas: 8\n      update_config:\n        parallelism: 2\n        delay: 20s\n      restart_policy:\n        condition: on-failure  \n...\n\n\n\n\nDeploy  using  the same command as earlier,\n\n\ndocker stack deploy --compose-file stack.yml instavote\n\n\n\n\nFault Tolerance\n\n\n\n\nDelete a node\n\n\nObserve the node being removed from cluster\n\n\nObserve tasks getting rescheduled automatically on available nodes", 
            "title": "Docker SWARM Quick Dive"
        }, 
        {
            "location": "/swarm-quickdive/#lab-docker-swarm-quick-dive", 
            "text": "Create a 5 nodes (3 masters, 2nodes) swarm cluster using http://play-with-docker.com", 
            "title": "Lab: Docker SWARM Quick Dive"
        }, 
        {
            "location": "/swarm-quickdive/#launch-a-visualizer-on-master-swarm-manager", 
            "text": "docker run -itd -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock schoolofdevops/visualizer", 
            "title": "Launch a Visualizer on Master (SWARM Manager)"
        }, 
        {
            "location": "/swarm-quickdive/#deploying-service-with-swarm-the-imperative-way", 
            "text": "docker service create --name vote schoolofdevops/vote  docker service ls\ndocker service inspect  docker service  update --publish-add 80:80 vote  Try accessing port 80 on any of the nodes in the swarm cluster to validate.", 
            "title": "Deploying Service with swarm - The imperative way"
        }, 
        {
            "location": "/swarm-quickdive/#scaling-a-service", 
            "text": "docker service scale vote=4\ndocker service  ls\ndocker service scale vote=2", 
            "title": "Scaling a service"
        }, 
        {
            "location": "/swarm-quickdive/#cleaning-up", 
            "text": "docker service rm vote", 
            "title": "Cleaning Up"
        }, 
        {
            "location": "/swarm-quickdive/#orchestrating-applications-with-stack-deploy", 
            "text": "file: stack.yml  version:  3 \n\nnetworks:\n  nw01:\n    driver: overlay\n\nvolumes:\n  db-data:\n\nservices:\n  vote:\n    image: schoolofdevops/vote:v1\n    ports:\n      - 80\n    networks:\n      - nw01\n    depends_on:\n      - redis\n    deploy:\n      replicas: 8\n      update_config:\n        parallelism: 2\n        delay: 20s\n      restart_policy:\n        condition: on-failure  \n\n  redis:\n    image: redis:alpine\n    networks:\n      - nw01\n\n  worker:\n    image: schoolofdevops/vote-worker\n    networks:\n      - nw01\n    depends_on:\n      - redis\n      - db\n\n  db:\n    image: postgres:9.4\n    networks:\n      - nw01\n    volumes:\n      - db-data:/var/lib/postgresql/data\n\n  result:\n    image: schoolofdevops/vote-result\n    ports:\n      - 5001:80\n    networks:\n      - nw01\n    depends_on:\n      - db  You could also copy the above file using the followinng command,  wget -chttps://gist.githubusercontent.com/initcron/8a5ebd534df74ab2a83e96218b56137d/raw/9e748637aed121b67ceddeca8678750596c81ab7/stack.yml  Deploy a stack  docker stack deploy --compose-file stack.yml instavote  Validate  docker stack ls\n\ndocker stack services instavote\n\ndocker service ls\n\ndocker service scale instavote_vote=4", 
            "title": "Orchestrating Applications with Stack Deploy"
        }, 
        {
            "location": "/swarm-quickdive/#deploying-a-new-version", 
            "text": "Update stack.yml with the new version of the image  ....\nservices:\n  vote:\n    image: schoolofdevops/vote:v2\n  .....\n\n    deploy:\n      replicas: 8\n      update_config:\n        parallelism: 2\n        delay: 20s\n      restart_policy:\n        condition: on-failure  \n...  Deploy  using  the same command as earlier,  docker stack deploy --compose-file stack.yml instavote", 
            "title": "Deploying a new version"
        }, 
        {
            "location": "/swarm-quickdive/#fault-tolerance", 
            "text": "Delete a node  Observe the node being removed from cluster  Observe tasks getting rescheduled automatically on available nodes", 
            "title": "Fault Tolerance"
        }, 
        {
            "location": "/swarm-networking-deepdive/", 
            "text": "SWARM Networking Deep Dive\n\n\nIn this module, we are going to set on a interesting journey of how SWARM netwoking functions under the hood. We will delving deeper in the world of bridges, vxlans, overlays, underlays, kernel ipvs and follow the journey of a packet in a swarm cluster. We will also be looking into how docker leverages iptables and ipvs, both kernel features, to implement the service discovery and load balancing. \n\n\nInstalling pre reqs\n\n\nInstall bridge utils\n\n\napt-get install bridge-utils\n\n\n\n\nExamine the networks before setting up Swarm\n\n\nbrctl show\n\n\n\n\n\nbridge name bridge id       STP enabled interfaces\n\ndocker0     8000.024268987cd3   no\n\n\n\n\ndocker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\n896388d51d18        bridge              bridge              local\n3e3e8fec9527        host                host                local\n385a6e374d9d        none                null                local\n\n\n\n\nExamine the network configurations created by SWARM\n\n\nList the networks\n\n\ndocker network ls\n\nNETWORK ID          NAME                DRIVER              SCOPE\n9b3cdad15a64        bridge              bridge              local\n71ad6ab6c0fb        docker_gwbridge     bridge              local\n6d42f614ce37        host                host                local\nlpq3tzoevynh        ingress             overlay             swarm\nce30767f4305        none                null                local\n\n\n\n\nwhere,\n\n\ndocker_gwbridge : bridge network created by swarm to connect containers to host and outside world\n\n\ningress: overlay network created by swarm for external service discovery, load balancing with routing mesh\n\n\nExamine the overlay vxlan inmplemntation\n\n\nInspect networks\n\n\ndocker network inspect docker_gwbridge\n\n\n\n\n[output]\n\n\n       \nContainers\n: {\n            \ningress-sbox\n: {\n                \nName\n: \ngateway_ingress-sbox\n,\n                \nEndpointID\n: \nb735335b753af4222fa253ba8496fe5a9bff10f8ddc698bd938d2b3e10780d54\n,\n                \nMacAddress\n: \n02:42:ac:12:00:02\n,\n                \nIPv4Address\n: \n172.18.0.2/16\n,\n                \nIPv6Address\n: \n\n            }\n        },\n\n\n\n\nwhere,\n ingress-sbox : sandbox created with network namespace and configs, purely for service discovery and load balancing\n\n\nEndpointID   : endpoing created (veth pair) in ingress-sbox e.g. eth0 inside this network namespace\n\n\ndocker network inspect ingress\n\n\n\n\n[output]\n\n\n      \nContainers\n: {\n            \ningress-sbox\n: {\n                \nName\n: \ningress-endpoint\n,\n                \nEndpointID\n: \na187751fda1c95b0f9c47bfe5d4104cf5195a839fef588bc7e3b02da5972ca7a\n,\n                \nMacAddress\n: \n02:42:0a:ff:00:02\n,\n                \nIPv4Address\n: \n10.255.0.2/16\n,\n                \nIPv6Address\n: \n\n            }\n        },\n        \nOptions\n: {\n            \ncom.docker.network.driver.overlay.vxlanid_list\n: \n4096\n\n        },\n        \nLabels\n: {},\n        \nPeers\n: [\n            {\n                \nName\n: \n02dddbfc3e9a\n,\n                \nIP\n: \n159.65.167.88\n\n            },\n            {\n                \nName\n: \n96473fed4b7c\n,\n                \nIP\n: \n159.89.42.230\n\n            },\n            {\n                \nName\n: \nc92920c69b92\n,\n                \nIP\n: \n159.89.41.130\n\n            }\n        ]\n    }\n\n\n\n\nwhere,\n\n\ningress-sbox : sandbox created with network namespace and configs, purely for service discovery and load balancing\n\n\nEndpointID   : endpoing created (veth pair) in ingress-sbox\n\n\nPeers        : nodes participating in this overlay\n\n\n4096         : VXLAN ID\n\n\nExamine the ingress-sbox namespace\n\n\nLearn about netshoot utility at https://github.com/nicolaka/netshoot\n\n\nLaunch \nnetshoot\n container, and connect to \ningress-sbox\n using nsenter.\n\n\ndocker run -it --rm -v /var/run/docker/netns:/netns --privileged=true nicolaka/netshoot nsenter --net=/netns/ingress_sbox sh\n\n\n\n\nFrom inside netshoot container,\n\n\nifconfig\nip link show\n\n\n\n\n\n[output]\n\n\n1: lo: \nLOOPBACK,UP,LOWER_UP\n mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n9: eth0: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1450 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:0a:ff:00:02 brd ff:ff:ff:ff:ff:ff\n12: eth1: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff\n\n\n\n\nOn the host\n\n\nip link show\n\n\n\n\n[output]\n\n\n1: lo: \nLOOPBACK,UP,LOWER_UP\n mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 92:20:8a:88:b6:e8 brd ff:ff:ff:ff:ff:ff\n3: docker0: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:68:98:7c:d3 brd ff:ff:ff:ff:ff:ff\n7: ov-001000-lpq3t: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1450 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 8a:17:46:93:46:94 brd ff:ff:ff:ff:ff:ff\n8: vx-001000-lpq3t: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1450 qdisc noqueue master ov-001000-lpq3t state UNKNOWN mode DEFAULT group default\n    link/ether 8a:17:46:93:46:94 brd ff:ff:ff:ff:ff:ff\n10: veth28c87ba: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1450 qdisc noqueue master ov-001000-lpq3t state UP mode DEFAULT group default\n    link/ether 8a:24:17:29:46:a7 brd ff:ff:ff:ff:ff:ff\n11: docker_gwbridge: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:a0:ca:1d:96 brd ff:ff:ff:ff:ff:ff\n13: veth0740008: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default\n    link/ether aa:b8:35:c2:97:74 brd ff:ff:ff:ff:ff:ff\n19: veth97a403d: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default\n    link/ether 8a:80:a4:17:3b:2d brd ff:ff:ff:ff:ff:ff\n\n\n\n\nIf you compare two  outputs above,\n\n\n9  \n=====\n 10   : ingress network\n\n\n12 \n=====\n 13   : docker_gwbridge network\n\n\nThese are then further bridged. Examine the bridges in the next part.\n\n\nInterfaces and bridges\n\n\nifconfig\nbrctl show\n\n\n\n\n[output]\n\n\nbrctl show\nbridge name bridge id       STP enabled interfaces\ndocker0     8000.02425dcabce4   no      veth3850215\ndocker_gwbridge     8000.0242105642b6   no      veth4dae0de\nov-001000-wo0i1     8000.1e8f6f3278a0   no      vethc978c4b\n                            vx-001000-wo0i1\n\n\n\n\nNote down the vx-001000-wo0i1 id.  To check more information use the following command.   \n\n\n[ Replace the command with your VXLAN ID ]\n\n\nip -d link show vx-001000-wo0i1\n\n\n\n\nShow forwarding table\n\n\nbridge fdb show dev vx-001000-wo0i1\n\n\n\n\n[output]\n\n\n5e:20:18:b1:1d:0e vlan 0 permanent\n02:42:0a:ff:00:03 dst 159.89.39.105 self permanent\n02:42:0a:ff:00:04 dst 165.227.64.215 self permanent\n\n\n\n\nwhere,\n\n\n5e:20:18:b1:1d:0e =\n  mac of the current host\n\n02:42:2c:32:94:4e =\n   mac id of ingress_box endpoint for ingress network on host with ip 159.89.39.105\n02:42:b2:0d:24:f8 =\n  mac id of ingress_box endpoint for ingress network on host with ip 165.227.64.215\n\n\nExamine the traffic\n\n\nTraffic on 2377/tcp : Cluster management communication\n\n\ntcpdump -v -i eth0 port 2377\n\n\n\n\nInter node gossip\n\n\ntcpdump -v -i eth0 port 7946\n\n\n\n\nData plan traffic on overlay\n\n\ntcpdump -v -i eth0 udp and port 4789\n\n\n\n\nCreating overlay networks\n\n\ndocker network create -d overlay mynet0\ndocker network ls\ndocker network inspect mynet0\n\n\n\n\n\nExamine the options, its missing \nencrypted\n flag\n\n\n     \nConfigOnly\n: false,\n        \nContainers\n: null,\n        \nOptions\n: {\n            \ncom.docker.network.driver.overlay.vxlanid_list\n: \n4097\n\n        },\n        \nLabels\n: null\n\n\n\n\nwhere,\n  4097 :   vnid of this VXLAN\n\n\ndocker network create --opt encrypted -d overlay vote\ndocker network ls\ndocker network inspect vote\n\n\n\n\n\nthis time, encryption is enabled\n\n\n      \nContainers\n: null,\n        \nOptions\n: {\n            \ncom.docker.network.driver.overlay.vxlanid_list\n: \n4098\n,\n            \nencrypted\n: \n\n        },\n        \nLabels\n: null\n    }\n\n\n\n\n Try This\n\n\nObserve the following by listing networks on all nodes,   \n\n\ndocker network ls\n\n\n\n\n\n\nall manager nodes have the new overlay network\n\n\nworker nodes will create it on need basis, only if there is a task running on that node\n\n\n\n\nLets learn what all is created with this overlay network,\n\n\nifconfig\nbrctl show\n\n\n\n\nLaunch Service with overlay network\n\n\ndocker service ls\ndocker service create --name redis --network vote --replicas=2 redis:alpine\n\n\n\n\n[output]\n\n\n8mxs1phssydpwi23teifpqcwr\noverall progress: 2 out of 2 tasks\n1/2: running   [==================================================\n]\n2/2: running   [==================================================\n]\nverify: Service converged\n\n\n\n\nCheck network on all nodes. It would be created only on selective nodes where tasks are scheduled\n\n\ndocker network ls\ndocker network inspect vote\n\n\n\n\ndocker ps\n\n\n4ea9c75179c3        redis:alpine        \ndocker-entrypoint.s\u2026\n   About a minute ago   Up About a minute   6379/tcp            redis.2.pbyb0o2e60gm1ozwc3wz9f7ou\n\n\n\n\nCorrelate interfaces and trace it\n\n\nInside the container\n\n\ndocker exec 4ea9c75179c3 ip link\n\n1: lo: \nLOOPBACK,UP,LOWER_UP\n mtu 65536 qdisc noqueue state UNKNOWN\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n16: eth0: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1424 qdisc noqueue state UP\n    link/ether 02:42:0a:00:00:07 brd ff:ff:ff:ff:ff:ff\n18: eth1: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc noqueue state UP\n    link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff\n\n\n\n\nand on the host\n\n\nroot@swarm-01:/var/run# ip link\n1: lo: \nLOOPBACK,UP,LOWER_UP\n mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 9e:28:4c:8a:bf:5d brd ff:ff:ff:ff:ff:ff\n3: docker0: \nNO-CARRIER,BROADCAST,MULTICAST,UP\n mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default\n    link/ether 02:42:5d:ca:bc:e4 brd ff:ff:ff:ff:ff:ff\n7: ov-001000-wo0i1: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1450 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 1e:8f:6f:32:78:a0 brd ff:ff:ff:ff:ff:ff\n8: vx-001000-wo0i1: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1450 qdisc noqueue master ov-001000-wo0i1 state UNKNOWN mode DEFAULT group default\n    link/ether 5e:20:18:b1:1d:0e brd ff:ff:ff:ff:ff:ff\n10: vethc978c4b: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1450 qdisc noqueue master ov-001000-wo0i1 state UP mode DEFAULT group default\n    link/ether 1e:8f:6f:32:78:a0 brd ff:ff:ff:ff:ff:ff\n11: docker_gwbridge: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:10:56:42:b6 brd ff:ff:ff:ff:ff:ff\n13: veth4dae0de: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default\n    link/ether 26:ea:d2:47:25:0d brd ff:ff:ff:ff:ff:ff\n14: ov-001001-7672d: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1424 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 42:a3:4b:f3:ca:05 brd ff:ff:ff:ff:ff:ff\n15: vx-001001-7672d: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1424 qdisc noqueue master ov-001001-7672d state UNKNOWN mode DEFAULT group default\n    link/ether 42:a3:4b:f3:ca:05 brd ff:ff:ff:ff:ff:ff\n17: veth4dd295a: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1424 qdisc noqueue master ov-001001-7672d state UP mode DEFAULT group default\n    link/ether ba:83:a3:3c:73:b1 brd ff:ff:ff:ff:ff:ff\n19: veth78165ba: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default\n    link/ether 76:9a:35:13:b3:64 brd ff:ff:ff:ff:ff:ff\n\n\n\n\nveth Pairs\n\n\n16: eth0  \n===\n  17: veth4dd295a (Overlay ov-001001-7672d)\n\n\n18: eth1  \n===\n  19: veth78165ba (docker_gwbridge)\n\n\nShow forwarding table for this overlay vtep\n\n\nbrctl show\nbridge fdb show dev vx-001001-7672d\n\n\n\n\n[output]\n\n\n42:a3:4b:f3:ca:05 vlan 0 permanent\n02:42:0a:00:00:06 dst 159.89.39.105 self permanent\n\n\n\n\nWhere, 02:42:0a:00:00:06 should be the mac id of the container on the other hosts\n\n\ne.g. on swarm-2\n\n\nroot@swarm-02:~# docker exec 92ee739ecdca ip lin\n1: lo: \nLOOPBACK,UP,LOWER_UP\n mtu 65536 qdisc noqueue state UNKNOWN\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n16: eth0: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1424 qdisc noqueue state UP\n    link/ether 02:42:0a:00:00:06 brd ff:ff:ff:ff:ff:ff\n\n\nroot@swarm-02:~# ip link\n17: veth353ea84: \nBROADCAST,MULTICAST,UP,LOWER_UP\n mtu 1424 qdisc noqueue master ov-001001-7672d state UP mode DEFAULT group default\n    link/ether 6e:2a:bb:f1:52:2a brd ff:ff:ff:ff:ff:ff\n\n\n\n\n\nTask: Do the same from the other end. Check the fdb for vxlan interface on swarm-02 and correlate it with the mac id of a container running on swarm-01\n\n\nLaunch worker service in same overlay\n\n\ndocker service create --name worker --network vote schoolofdevops/worker\n\n\n\n\nShould get launched on the third node  as the default scheduling algorithm is to sread the load evenly.\n\n\nNow on node1 and node2 check the fdb again, should see a new vtep endpoint\n\n\ne.g.\n\n\n[replace vx-001001-7672d with the id of the vxlan interface created for this overlayn/w, get it by using brctl show ]\n\n\n\nbridge fdb show dev vx-001001-7672d\n22:ba:86:43:71:27 vlan 0 permanent\n02:42:0a:00:00:07 dst 159.65.161.208 self permanent\n02:42:0a:00:00:09 dst 165.227.64.215 self permanent\n\n\n\n\nScale redis service\n\n\ndocker service scale redis=5\n\n\n\n\nExamine the fdb again\n\n\nbridge fdb show dev vx-001001-7672d\n42:a3:4b:f3:ca:05 vlan 0 permanent\n02:42:0a:00:00:09 vlan 0\n02:42:0a:00:00:06 dst 159.89.39.105 self permanent\n02:42:0a:00:00:09 dst 165.227.64.215 self permanent\n02:42:0a:00:00:0a dst 165.227.64.215 self permanent\n02:42:0a:00:00:0b dst 159.89.39.105 self permanent\n\n\n\n\nwhere,\n\n\nthe table shows entries for every other\n\n\nUnderlying VXLAN service  and traffic\n\n\nport 4789 is reservered for vxlan. Packets will have headers with this.\n\n\nnetstat -pan | grep  4789\n\n\n\n\n\nTo see the packets going through the vxlan interface\n\n\nbrctl show\ntcpdump -i ov-001001-7672d\n\n\n\n\nInternal Load Balancing\n\n\nconnect to one of the redis instances on one of the nodes\n\n\ndocker ps\n\ndocker run --rm -it --net container:0b0309771045 --privileged nicolaka/netshoot\n\n\n\n\n\nVerify redirect to ipvs\n\n\niptables -nvL -t nat\n\n\n\n\n\nChain POSTROUTING (policy ACCEPT 85 packets, 5426 bytes)\n pkts bytes target     prot opt in     out     source               destination\n   59  3712 DOCKER_POSTROUTING  all  --  *      *       0.0.0.0/0            127.0.0.11\n    3   180 SNAT       all  --  *      *       0.0.0.0/0            10.0.0.0/24          ipvs to:10.0.0.11\n\n\n\n\n\n\nwhere,\n\n\nipvs to:10.0.0.11 . : is routing the traffic to ipvs, running on the  same container\n\n\ncheck the mangle markers\n\n\niptables -nvL -t mangle\n\n\n\n\nChain OUTPUT (policy ACCEPT 864 packets, 62611 bytes)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 MARK       all  --  *      *       0.0.0.0/0            10.0.0.5             MARK set 0x100\n  168 14112 MARK       all  --  *      *       0.0.0.0/0            10.0.0.8             MARK set 0x101\n   26  1757 MARK       all  --  *      *       0.0.0.0/0            10.0.0.13            MARK set 0x103\n\n\n\n\n\nwhere,\n10.0.0.5 is s VIP for service \nxyz\n . 0x100is a HEX for 256.\n\n\nto check where this is redirecting, look at the ipvs rules\n\n\n# ipvsadm\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -\n RemoteAddress:Port           Forward Weight ActiveConn InActConn\nFWM  256 rr\n  -\n redis.1.m911y2nt3wy0qo5x8i9p Masq    1      0          0\n  -\n redis.2.pbyb0o2e60gm1ozwc3wz Masq    1      0          0\n  -\n e89fbe75fb1a.vote:0          Masq    1      0          0\n  -\n 0b0309771045:0               Masq    1      0          0\n  -\n redis.5.oag34plamnmptoby9b0y Masq    1      0          0\nFWM  257 rr\n  -\n worker.1.lhxvgjgn5k6soaksifz Masq    1      0          0\nFWM  259 rr\n  -\n vote.1.hhv9l10vb5yocxmkbzzdv Masq    1      0          0\n  -\n vote.2.h1iwvk5t8hr9pc6mpqsxk Masq    1      0          0\n\n\n\n\nhere the following is doing a RR load balancing across 5 nodes\n\n\nFWM  256 rr\n  -\n redis.1.m911y2nt3wy0qo5x8i9p Masq    1      0          0\n  -\n redis.2.pbyb0o2e60gm1ozwc3wz Masq    1      0          0\n  -\n e89fbe75fb1a.vote:0          Masq    1      0          0\n  -\n 0b0309771045:0               Masq    1      0          0\n  -\n redis.5.oag34plamnmptoby9b0y Masq    1      0          0\n\n\n\n\nPort Publishing, Routing Mesh, Ingress Network and External Service Discovery\n\n\ndocker network ls\ndocker network inspect ingress\n\n\n\n\nwhere,\n\n\nPeers : shows all the hosts which are part of this ingress (note the peers and corraborate)\n Containers : shows ingress-sbox namespace (its not a containers, just a namespace, has one interface in gwbridge, another ingress)\n\n\nCreat a container which is part of this ingress\n\n\ndocker service create --name vote --network vote --publish 80 --replicas=2 schoolofdevops/vote\n\n\n\n\ndocker ps\n\n\n[output]\n\n\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES\n85642d7fa2f4        schoolofdevops/vote:latest   \ngunicorn app:app -b\ufffd\n   28 seconds ago      Up 27 seconds       80/tcp              vote.1.hhv9l10vb5yocxmkbzzdvtmj2\n64f05b29e559        redis:alpine                 \ndocker-entrypoint.s\ufffd\n   29 minutes ago      Up 29 minutes       6379/tcp            redis.5.oag34plamnmptoby9b0yuaooi\n4ea9c75179c3        redis:alpine                 \ndocker-entrypoint.s\ufffd\n   About an hour ago   Up About an hour    6379/tcp            redis.2.pbyb0o2e60gm1ozwc3wz9f7ou\n\n\n\n\nConnect to container and examine\n\n\ndocker exec 85642d7fa2f4 ifconfig\n\ndocker exec 85642d7fa2f4 ip link show\n\ndocker exec 85642d7fa2f4 netstat -nr\n\n\n\n\n\nCorrelate it with the host veth pair\n\n\nip link show\nbrctl show\ndocker network ls\n\n\n\n\neth0 =\n ingress\neth1 =\n gwbridge\n\n\neth2 =\n overlay for apps\n\n\nService Networking and Routing Mesh\n\n\nFor external facing ingress connnetiion, service routing works this way,\n\n\ningress ==\n gwbridge ==\n ingress-sbox (its just a n/w namespae not a container) ==\n ipvs ==\n underlay\n\n\n\n\nCheck iptable rules on the host\n\n\n\n\niptables -nvL -t nat\n\n\n\n\n[output]\n\n\n...\nChain DOCKER-INGRESS (2 references)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:30000 to:172.18.0.2:30000\n   31  1744 RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0\n\n\n\n\nwhere,\n\n\ntcp dpt:30000 to:172.18.0.2:30000 =\n is forwarding  the traffic received on 30000 port to 172.18.0.2:30000. Here 172.18.0.2 belongs to \ningress_sbox\n so whatever happens next is inside there...\n\n\n\n\nConnecting to ingress-sbox\n\n\n\n\ndocker run -it --rm -v /var/run/docker/netns:/var/run/docker/netns --privileged=true nicolaka/netshoot\n\nnsenter --net=/var/run/docker/netns/ingress_sbox sh\n\n\n\n\n\nalternately\n\n\ndocker run -it --rm -v /var/run/docker/netns:/netns --privileged=true nicolaka/netshoot nsenter --net=/netns/ingress_sbox sh\n\n\n\n\n\nand then\n\n\niptables -nvL -t mangle\n\n\n\n\nChain PREROUTING (policy ACCEPT 16 packets, 1888 bytes)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 MARK       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:30000 MARK set 0x102\n\n\n\n\n\nwhere,\n\n\niptables is setting MARK to 0x102 for anything that comes in on 30000 port. 0x102 is hex value and can be translated into integer from here https://www.binaryhexconverter.com/hex-to-decimal-converter\n\n\ne.g.  0x102 = 258\n\n\n\n\nNow check the rules for above mark with ipvs\n\n\n\n\nipvsadm\n\n\n\n\n\n[output]\n\n\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -\n RemoteAddress:Port           Forward Weight ActiveConn InActConn\nFWM  258 rr\n  -\n 10.255.0.6:0                 Masq    1      0          0\n  -\n 10.255.0.7:0                 Masq    1      0          0\n\n\n\n\nThis is where the decision is made as to where this packet goes. Since ipvs uses round robin algorithm, one of these ips are selected and then packet is sent over the \ningress\n overlay network.\n\n\nFinally,\n\n\nTo see the traffic on ingress network\n\n\non node2\n\n\n\ntcpdump -i eth0 udp and port 4789\ntcpdump -i eth0 esp\n\n\n\n\n\n\n\nAdditional Commands\n\n\ntail -f syslog\ntcpdump -i eth0 udp and port 4789\ntcpdump -i eth0 esp\nip addr\nip link\niptables -t nat -nvL\n\n\n\n\n\n\n\nTo see namespaces on the docker host\n\n\n\n\ncd /var/run\nln -s /var/run/docker/netns netns\nip netns\n\n\ndocker network ls\n[company network and ns ids]", 
            "title": "SWARM Networking Deep Dive"
        }, 
        {
            "location": "/swarm-networking-deepdive/#swarm-networking-deep-dive", 
            "text": "In this module, we are going to set on a interesting journey of how SWARM netwoking functions under the hood. We will delving deeper in the world of bridges, vxlans, overlays, underlays, kernel ipvs and follow the journey of a packet in a swarm cluster. We will also be looking into how docker leverages iptables and ipvs, both kernel features, to implement the service discovery and load balancing.", 
            "title": "SWARM Networking Deep Dive"
        }, 
        {
            "location": "/swarm-networking-deepdive/#installing-pre-reqs", 
            "text": "Install bridge utils  apt-get install bridge-utils", 
            "title": "Installing pre reqs"
        }, 
        {
            "location": "/swarm-networking-deepdive/#examine-the-networks-before-setting-up-swarm", 
            "text": "brctl show  bridge name bridge id       STP enabled interfaces\n\ndocker0     8000.024268987cd3   no  docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\n896388d51d18        bridge              bridge              local\n3e3e8fec9527        host                host                local\n385a6e374d9d        none                null                local", 
            "title": "Examine the networks before setting up Swarm"
        }, 
        {
            "location": "/swarm-networking-deepdive/#examine-the-network-configurations-created-by-swarm", 
            "text": "List the networks  docker network ls\n\nNETWORK ID          NAME                DRIVER              SCOPE\n9b3cdad15a64        bridge              bridge              local\n71ad6ab6c0fb        docker_gwbridge     bridge              local\n6d42f614ce37        host                host                local\nlpq3tzoevynh        ingress             overlay             swarm\nce30767f4305        none                null                local  where,  docker_gwbridge : bridge network created by swarm to connect containers to host and outside world  ingress: overlay network created by swarm for external service discovery, load balancing with routing mesh  Examine the overlay vxlan inmplemntation", 
            "title": "Examine the network configurations created by SWARM"
        }, 
        {
            "location": "/swarm-networking-deepdive/#inspect-networks", 
            "text": "docker network inspect docker_gwbridge  [output]          Containers : {\n             ingress-sbox : {\n                 Name :  gateway_ingress-sbox ,\n                 EndpointID :  b735335b753af4222fa253ba8496fe5a9bff10f8ddc698bd938d2b3e10780d54 ,\n                 MacAddress :  02:42:ac:12:00:02 ,\n                 IPv4Address :  172.18.0.2/16 ,\n                 IPv6Address :  \n            }\n        },  where,\n ingress-sbox : sandbox created with network namespace and configs, purely for service discovery and load balancing  EndpointID   : endpoing created (veth pair) in ingress-sbox e.g. eth0 inside this network namespace  docker network inspect ingress  [output]         Containers : {\n             ingress-sbox : {\n                 Name :  ingress-endpoint ,\n                 EndpointID :  a187751fda1c95b0f9c47bfe5d4104cf5195a839fef588bc7e3b02da5972ca7a ,\n                 MacAddress :  02:42:0a:ff:00:02 ,\n                 IPv4Address :  10.255.0.2/16 ,\n                 IPv6Address :  \n            }\n        },\n         Options : {\n             com.docker.network.driver.overlay.vxlanid_list :  4096 \n        },\n         Labels : {},\n         Peers : [\n            {\n                 Name :  02dddbfc3e9a ,\n                 IP :  159.65.167.88 \n            },\n            {\n                 Name :  96473fed4b7c ,\n                 IP :  159.89.42.230 \n            },\n            {\n                 Name :  c92920c69b92 ,\n                 IP :  159.89.41.130 \n            }\n        ]\n    }  where,  ingress-sbox : sandbox created with network namespace and configs, purely for service discovery and load balancing  EndpointID   : endpoing created (veth pair) in ingress-sbox  Peers        : nodes participating in this overlay  4096         : VXLAN ID", 
            "title": "Inspect networks"
        }, 
        {
            "location": "/swarm-networking-deepdive/#examine-the-ingress-sbox-namespace", 
            "text": "Learn about netshoot utility at https://github.com/nicolaka/netshoot  Launch  netshoot  container, and connect to  ingress-sbox  using nsenter.  docker run -it --rm -v /var/run/docker/netns:/netns --privileged=true nicolaka/netshoot nsenter --net=/netns/ingress_sbox sh  From inside netshoot container,  ifconfig\nip link show  [output]  1: lo:  LOOPBACK,UP,LOWER_UP  mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n9: eth0:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1450 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:0a:ff:00:02 brd ff:ff:ff:ff:ff:ff\n12: eth1:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff  On the host  ip link show  [output]  1: lo:  LOOPBACK,UP,LOWER_UP  mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 92:20:8a:88:b6:e8 brd ff:ff:ff:ff:ff:ff\n3: docker0:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:68:98:7c:d3 brd ff:ff:ff:ff:ff:ff\n7: ov-001000-lpq3t:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1450 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 8a:17:46:93:46:94 brd ff:ff:ff:ff:ff:ff\n8: vx-001000-lpq3t:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1450 qdisc noqueue master ov-001000-lpq3t state UNKNOWN mode DEFAULT group default\n    link/ether 8a:17:46:93:46:94 brd ff:ff:ff:ff:ff:ff\n10: veth28c87ba:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1450 qdisc noqueue master ov-001000-lpq3t state UP mode DEFAULT group default\n    link/ether 8a:24:17:29:46:a7 brd ff:ff:ff:ff:ff:ff\n11: docker_gwbridge:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:a0:ca:1d:96 brd ff:ff:ff:ff:ff:ff\n13: veth0740008:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default\n    link/ether aa:b8:35:c2:97:74 brd ff:ff:ff:ff:ff:ff\n19: veth97a403d:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default\n    link/ether 8a:80:a4:17:3b:2d brd ff:ff:ff:ff:ff:ff  If you compare two  outputs above,  9   =====  10   : ingress network  12  =====  13   : docker_gwbridge network  These are then further bridged. Examine the bridges in the next part.", 
            "title": "Examine the ingress-sbox namespace"
        }, 
        {
            "location": "/swarm-networking-deepdive/#interfaces-and-bridges", 
            "text": "ifconfig\nbrctl show  [output]  brctl show\nbridge name bridge id       STP enabled interfaces\ndocker0     8000.02425dcabce4   no      veth3850215\ndocker_gwbridge     8000.0242105642b6   no      veth4dae0de\nov-001000-wo0i1     8000.1e8f6f3278a0   no      vethc978c4b\n                            vx-001000-wo0i1  Note down the vx-001000-wo0i1 id.  To check more information use the following command.     [ Replace the command with your VXLAN ID ]  ip -d link show vx-001000-wo0i1  Show forwarding table  bridge fdb show dev vx-001000-wo0i1  [output]  5e:20:18:b1:1d:0e vlan 0 permanent\n02:42:0a:ff:00:03 dst 159.89.39.105 self permanent\n02:42:0a:ff:00:04 dst 165.227.64.215 self permanent  where,  5e:20:18:b1:1d:0e =   mac of the current host \n02:42:2c:32:94:4e =    mac id of ingress_box endpoint for ingress network on host with ip 159.89.39.105\n02:42:b2:0d:24:f8 =   mac id of ingress_box endpoint for ingress network on host with ip 165.227.64.215", 
            "title": "Interfaces and bridges"
        }, 
        {
            "location": "/swarm-networking-deepdive/#examine-the-traffic", 
            "text": "Traffic on 2377/tcp : Cluster management communication  tcpdump -v -i eth0 port 2377  Inter node gossip  tcpdump -v -i eth0 port 7946  Data plan traffic on overlay  tcpdump -v -i eth0 udp and port 4789", 
            "title": "Examine the traffic"
        }, 
        {
            "location": "/swarm-networking-deepdive/#creating-overlay-networks", 
            "text": "docker network create -d overlay mynet0\ndocker network ls\ndocker network inspect mynet0  Examine the options, its missing  encrypted  flag        ConfigOnly : false,\n         Containers : null,\n         Options : {\n             com.docker.network.driver.overlay.vxlanid_list :  4097 \n        },\n         Labels : null  where,\n  4097 :   vnid of this VXLAN  docker network create --opt encrypted -d overlay vote\ndocker network ls\ndocker network inspect vote  this time, encryption is enabled         Containers : null,\n         Options : {\n             com.docker.network.driver.overlay.vxlanid_list :  4098 ,\n             encrypted :  \n        },\n         Labels : null\n    }   Try This  Observe the following by listing networks on all nodes,     docker network ls   all manager nodes have the new overlay network  worker nodes will create it on need basis, only if there is a task running on that node   Lets learn what all is created with this overlay network,  ifconfig\nbrctl show", 
            "title": "Creating overlay networks"
        }, 
        {
            "location": "/swarm-networking-deepdive/#launch-service-with-overlay-network", 
            "text": "docker service ls\ndocker service create --name redis --network vote --replicas=2 redis:alpine  [output]  8mxs1phssydpwi23teifpqcwr\noverall progress: 2 out of 2 tasks\n1/2: running   [================================================== ]\n2/2: running   [================================================== ]\nverify: Service converged  Check network on all nodes. It would be created only on selective nodes where tasks are scheduled  docker network ls\ndocker network inspect vote  docker ps  4ea9c75179c3        redis:alpine         docker-entrypoint.s\u2026    About a minute ago   Up About a minute   6379/tcp            redis.2.pbyb0o2e60gm1ozwc3wz9f7ou  Correlate interfaces and trace it  Inside the container  docker exec 4ea9c75179c3 ip link\n\n1: lo:  LOOPBACK,UP,LOWER_UP  mtu 65536 qdisc noqueue state UNKNOWN\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n16: eth0:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1424 qdisc noqueue state UP\n    link/ether 02:42:0a:00:00:07 brd ff:ff:ff:ff:ff:ff\n18: eth1:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc noqueue state UP\n    link/ether 02:42:ac:12:00:03 brd ff:ff:ff:ff:ff:ff  and on the host  root@swarm-01:/var/run# ip link\n1: lo:  LOOPBACK,UP,LOWER_UP  mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000\n    link/ether 9e:28:4c:8a:bf:5d brd ff:ff:ff:ff:ff:ff\n3: docker0:  NO-CARRIER,BROADCAST,MULTICAST,UP  mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default\n    link/ether 02:42:5d:ca:bc:e4 brd ff:ff:ff:ff:ff:ff\n7: ov-001000-wo0i1:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1450 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 1e:8f:6f:32:78:a0 brd ff:ff:ff:ff:ff:ff\n8: vx-001000-wo0i1:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1450 qdisc noqueue master ov-001000-wo0i1 state UNKNOWN mode DEFAULT group default\n    link/ether 5e:20:18:b1:1d:0e brd ff:ff:ff:ff:ff:ff\n10: vethc978c4b:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1450 qdisc noqueue master ov-001000-wo0i1 state UP mode DEFAULT group default\n    link/ether 1e:8f:6f:32:78:a0 brd ff:ff:ff:ff:ff:ff\n11: docker_gwbridge:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 02:42:10:56:42:b6 brd ff:ff:ff:ff:ff:ff\n13: veth4dae0de:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default\n    link/ether 26:ea:d2:47:25:0d brd ff:ff:ff:ff:ff:ff\n14: ov-001001-7672d:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1424 qdisc noqueue state UP mode DEFAULT group default\n    link/ether 42:a3:4b:f3:ca:05 brd ff:ff:ff:ff:ff:ff\n15: vx-001001-7672d:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1424 qdisc noqueue master ov-001001-7672d state UNKNOWN mode DEFAULT group default\n    link/ether 42:a3:4b:f3:ca:05 brd ff:ff:ff:ff:ff:ff\n17: veth4dd295a:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1424 qdisc noqueue master ov-001001-7672d state UP mode DEFAULT group default\n    link/ether ba:83:a3:3c:73:b1 brd ff:ff:ff:ff:ff:ff\n19: veth78165ba:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1500 qdisc noqueue master docker_gwbridge state UP mode DEFAULT group default\n    link/ether 76:9a:35:13:b3:64 brd ff:ff:ff:ff:ff:ff  veth Pairs  16: eth0   ===   17: veth4dd295a (Overlay ov-001001-7672d)  18: eth1   ===   19: veth78165ba (docker_gwbridge)  Show forwarding table for this overlay vtep  brctl show\nbridge fdb show dev vx-001001-7672d  [output]  42:a3:4b:f3:ca:05 vlan 0 permanent\n02:42:0a:00:00:06 dst 159.89.39.105 self permanent  Where, 02:42:0a:00:00:06 should be the mac id of the container on the other hosts  e.g. on swarm-2  root@swarm-02:~# docker exec 92ee739ecdca ip lin\n1: lo:  LOOPBACK,UP,LOWER_UP  mtu 65536 qdisc noqueue state UNKNOWN\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n16: eth0:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1424 qdisc noqueue state UP\n    link/ether 02:42:0a:00:00:06 brd ff:ff:ff:ff:ff:ff\n\n\nroot@swarm-02:~# ip link\n17: veth353ea84:  BROADCAST,MULTICAST,UP,LOWER_UP  mtu 1424 qdisc noqueue master ov-001001-7672d state UP mode DEFAULT group default\n    link/ether 6e:2a:bb:f1:52:2a brd ff:ff:ff:ff:ff:ff  Task: Do the same from the other end. Check the fdb for vxlan interface on swarm-02 and correlate it with the mac id of a container running on swarm-01", 
            "title": "Launch Service with overlay network"
        }, 
        {
            "location": "/swarm-networking-deepdive/#launch-worker-service-in-same-overlay", 
            "text": "docker service create --name worker --network vote schoolofdevops/worker  Should get launched on the third node  as the default scheduling algorithm is to sread the load evenly.  Now on node1 and node2 check the fdb again, should see a new vtep endpoint  e.g.  [replace vx-001001-7672d with the id of the vxlan interface created for this overlayn/w, get it by using brctl show ]  \nbridge fdb show dev vx-001001-7672d\n22:ba:86:43:71:27 vlan 0 permanent\n02:42:0a:00:00:07 dst 159.65.161.208 self permanent\n02:42:0a:00:00:09 dst 165.227.64.215 self permanent", 
            "title": "Launch worker service in same overlay"
        }, 
        {
            "location": "/swarm-networking-deepdive/#scale-redis-service", 
            "text": "docker service scale redis=5  Examine the fdb again  bridge fdb show dev vx-001001-7672d\n42:a3:4b:f3:ca:05 vlan 0 permanent\n02:42:0a:00:00:09 vlan 0\n02:42:0a:00:00:06 dst 159.89.39.105 self permanent\n02:42:0a:00:00:09 dst 165.227.64.215 self permanent\n02:42:0a:00:00:0a dst 165.227.64.215 self permanent\n02:42:0a:00:00:0b dst 159.89.39.105 self permanent  where,  the table shows entries for every other", 
            "title": "Scale redis service"
        }, 
        {
            "location": "/swarm-networking-deepdive/#underlying-vxlan-service-and-traffic", 
            "text": "port 4789 is reservered for vxlan. Packets will have headers with this.  netstat -pan | grep  4789  To see the packets going through the vxlan interface  brctl show\ntcpdump -i ov-001001-7672d", 
            "title": "Underlying VXLAN service  and traffic"
        }, 
        {
            "location": "/swarm-networking-deepdive/#internal-load-balancing", 
            "text": "connect to one of the redis instances on one of the nodes  docker ps\n\ndocker run --rm -it --net container:0b0309771045 --privileged nicolaka/netshoot  Verify redirect to ipvs  iptables -nvL -t nat  \nChain POSTROUTING (policy ACCEPT 85 packets, 5426 bytes)\n pkts bytes target     prot opt in     out     source               destination\n   59  3712 DOCKER_POSTROUTING  all  --  *      *       0.0.0.0/0            127.0.0.11\n    3   180 SNAT       all  --  *      *       0.0.0.0/0            10.0.0.0/24          ipvs to:10.0.0.11  where,  ipvs to:10.0.0.11 . : is routing the traffic to ipvs, running on the  same container  check the mangle markers  iptables -nvL -t mangle  Chain OUTPUT (policy ACCEPT 864 packets, 62611 bytes)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 MARK       all  --  *      *       0.0.0.0/0            10.0.0.5             MARK set 0x100\n  168 14112 MARK       all  --  *      *       0.0.0.0/0            10.0.0.8             MARK set 0x101\n   26  1757 MARK       all  --  *      *       0.0.0.0/0            10.0.0.13            MARK set 0x103  where,\n10.0.0.5 is s VIP for service  xyz  . 0x100is a HEX for 256.  to check where this is redirecting, look at the ipvs rules  # ipvsadm\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -  RemoteAddress:Port           Forward Weight ActiveConn InActConn\nFWM  256 rr\n  -  redis.1.m911y2nt3wy0qo5x8i9p Masq    1      0          0\n  -  redis.2.pbyb0o2e60gm1ozwc3wz Masq    1      0          0\n  -  e89fbe75fb1a.vote:0          Masq    1      0          0\n  -  0b0309771045:0               Masq    1      0          0\n  -  redis.5.oag34plamnmptoby9b0y Masq    1      0          0\nFWM  257 rr\n  -  worker.1.lhxvgjgn5k6soaksifz Masq    1      0          0\nFWM  259 rr\n  -  vote.1.hhv9l10vb5yocxmkbzzdv Masq    1      0          0\n  -  vote.2.h1iwvk5t8hr9pc6mpqsxk Masq    1      0          0  here the following is doing a RR load balancing across 5 nodes  FWM  256 rr\n  -  redis.1.m911y2nt3wy0qo5x8i9p Masq    1      0          0\n  -  redis.2.pbyb0o2e60gm1ozwc3wz Masq    1      0          0\n  -  e89fbe75fb1a.vote:0          Masq    1      0          0\n  -  0b0309771045:0               Masq    1      0          0\n  -  redis.5.oag34plamnmptoby9b0y Masq    1      0          0", 
            "title": "Internal Load Balancing"
        }, 
        {
            "location": "/swarm-networking-deepdive/#port-publishing-routing-mesh-ingress-network-and-external-service-discovery", 
            "text": "docker network ls\ndocker network inspect ingress  where,  Peers : shows all the hosts which are part of this ingress (note the peers and corraborate)\n Containers : shows ingress-sbox namespace (its not a containers, just a namespace, has one interface in gwbridge, another ingress)  Creat a container which is part of this ingress  docker service create --name vote --network vote --publish 80 --replicas=2 schoolofdevops/vote  docker ps  [output]  CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS               NAMES\n85642d7fa2f4        schoolofdevops/vote:latest    gunicorn app:app -b\ufffd    28 seconds ago      Up 27 seconds       80/tcp              vote.1.hhv9l10vb5yocxmkbzzdvtmj2\n64f05b29e559        redis:alpine                  docker-entrypoint.s\ufffd    29 minutes ago      Up 29 minutes       6379/tcp            redis.5.oag34plamnmptoby9b0yuaooi\n4ea9c75179c3        redis:alpine                  docker-entrypoint.s\ufffd    About an hour ago   Up About an hour    6379/tcp            redis.2.pbyb0o2e60gm1ozwc3wz9f7ou  Connect to container and examine  docker exec 85642d7fa2f4 ifconfig\n\ndocker exec 85642d7fa2f4 ip link show\n\ndocker exec 85642d7fa2f4 netstat -nr  Correlate it with the host veth pair  ip link show\nbrctl show\ndocker network ls  eth0 =  ingress\neth1 =  gwbridge  eth2 =  overlay for apps", 
            "title": "Port Publishing, Routing Mesh, Ingress Network and External Service Discovery"
        }, 
        {
            "location": "/swarm-networking-deepdive/#service-networking-and-routing-mesh", 
            "text": "For external facing ingress connnetiion, service routing works this way,  ingress ==  gwbridge ==  ingress-sbox (its just a n/w namespae not a container) ==  ipvs ==  underlay   Check iptable rules on the host   iptables -nvL -t nat  [output]  ...\nChain DOCKER-INGRESS (2 references)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:30000 to:172.18.0.2:30000\n   31  1744 RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0  where,  tcp dpt:30000 to:172.18.0.2:30000 =  is forwarding  the traffic received on 30000 port to 172.18.0.2:30000. Here 172.18.0.2 belongs to  ingress_sbox  so whatever happens next is inside there...   Connecting to ingress-sbox   docker run -it --rm -v /var/run/docker/netns:/var/run/docker/netns --privileged=true nicolaka/netshoot\n\nnsenter --net=/var/run/docker/netns/ingress_sbox sh  alternately  docker run -it --rm -v /var/run/docker/netns:/netns --privileged=true nicolaka/netshoot nsenter --net=/netns/ingress_sbox sh  and then  iptables -nvL -t mangle  Chain PREROUTING (policy ACCEPT 16 packets, 1888 bytes)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 MARK       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:30000 MARK set 0x102  where,  iptables is setting MARK to 0x102 for anything that comes in on 30000 port. 0x102 is hex value and can be translated into integer from here https://www.binaryhexconverter.com/hex-to-decimal-converter  e.g.  0x102 = 258   Now check the rules for above mark with ipvs   ipvsadm  [output]  IP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -  RemoteAddress:Port           Forward Weight ActiveConn InActConn\nFWM  258 rr\n  -  10.255.0.6:0                 Masq    1      0          0\n  -  10.255.0.7:0                 Masq    1      0          0  This is where the decision is made as to where this packet goes. Since ipvs uses round robin algorithm, one of these ips are selected and then packet is sent over the  ingress  overlay network.  Finally,  To see the traffic on ingress network  on node2  \ntcpdump -i eth0 udp and port 4789\ntcpdump -i eth0 esp", 
            "title": "Service Networking and Routing Mesh"
        }, 
        {
            "location": "/swarm-networking-deepdive/#additional-commands", 
            "text": "tail -f syslog\ntcpdump -i eth0 udp and port 4789\ntcpdump -i eth0 esp\nip addr\nip link\niptables -t nat -nvL   To see namespaces on the docker host   cd /var/run\nln -s /var/run/docker/netns netns\nip netns  docker network ls\n[company network and ns ids]", 
            "title": "Additional Commands"
        }
    ]
}